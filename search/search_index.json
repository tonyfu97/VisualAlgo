{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VisualAlgo VisualAlgo is a collection of algorithms and models intersecting the domains of computer vision and neuroscience. The repository is categorized based on the major tasks/stages in visual processing. Each category includes relevant algorithms from both computer vision and neuroscience. Disclaimer This repository is created purely for educational purposes. It contains implementations of algorithms that might be copyrighted or protected by other legal means. The goal is to showcase these implementations as a personal project and to facilitate learning. Please note that while you are welcome to browse and learn from this code, I highly discourage using it directly in any production system or for any commercial purposes, as it may infringe upon the rights of the original algorithm authors or patent holders. For each algorithm implemented, aside from those widely used and commonly known, I have included citations and references to the original papers where they were presented. Always be sure to respect the rights of the original creators and acknowledge their work. It is your responsibility to ensure that you have the proper legal permission to use any code or concept presented here. This code is provided \"as is\" and without any warranties. The author of this repository is not responsible for any consequences that may arise from your use of the code. Please also note that this repository is currently a work in progress and may contain bugs.","title":"Home"},{"location":"#visualalgo","text":"VisualAlgo is a collection of algorithms and models intersecting the domains of computer vision and neuroscience. The repository is categorized based on the major tasks/stages in visual processing. Each category includes relevant algorithms from both computer vision and neuroscience.","title":"VisualAlgo"},{"location":"#disclaimer","text":"This repository is created purely for educational purposes. It contains implementations of algorithms that might be copyrighted or protected by other legal means. The goal is to showcase these implementations as a personal project and to facilitate learning. Please note that while you are welcome to browse and learn from this code, I highly discourage using it directly in any production system or for any commercial purposes, as it may infringe upon the rights of the original algorithm authors or patent holders. For each algorithm implemented, aside from those widely used and commonly known, I have included citations and references to the original papers where they were presented. Always be sure to respect the rights of the original creators and acknowledge their work. It is your responsibility to ensure that you have the proper legal permission to use any code or concept presented here. This code is provided \"as is\" and without any warranties. The author of this repository is not responsible for any consequences that may arise from your use of the code. Please also note that this repository is currently a work in progress and may contain bugs.","title":"Disclaimer"},{"location":"pages/attention-and-search/","text":"8. Attention and Search Computer Vision Algorithms Selective Search Neuroscience Models Feature Integration Theory Biased Competition Model","title":"8. Attention and Search"},{"location":"pages/attention-and-search/#8-attention-and-search","text":"","title":"8. Attention and Search"},{"location":"pages/attention-and-search/#computer-vision-algorithms","text":"Selective Search","title":"Computer Vision Algorithms"},{"location":"pages/attention-and-search/#neuroscience-models","text":"Feature Integration Theory Biased Competition Model","title":"Neuroscience Models"},{"location":"pages/color-vision/","text":"7. Color Vision Computer Vision Algorithms Gamma Correction Neuroscience Models Opponent Process Models Retinex Theory","title":"7. Color Vision"},{"location":"pages/color-vision/#7-color-vision","text":"","title":"7. Color Vision"},{"location":"pages/color-vision/#computer-vision-algorithms","text":"Gamma Correction","title":"Computer Vision Algorithms"},{"location":"pages/color-vision/#neuroscience-models","text":"Opponent Process Models Retinex Theory","title":"Neuroscience Models"},{"location":"pages/depth-perception-and-3d-reconstruction/","text":"6. Depth Perception and 3D Reconstruction Computer Vision Algorithms Stereo Vision Struction from Motion (SfM), SLAM Neuroscience Models Binocular Disparity Models Motion Parallax Models","title":"6. Depth Perception and 3D Reconstruction"},{"location":"pages/depth-perception-and-3d-reconstruction/#6-depth-perception-and-3d-reconstruction","text":"","title":"6. Depth Perception and 3D Reconstruction"},{"location":"pages/depth-perception-and-3d-reconstruction/#computer-vision-algorithms","text":"Stereo Vision Struction from Motion (SfM), SLAM","title":"Computer Vision Algorithms"},{"location":"pages/depth-perception-and-3d-reconstruction/#neuroscience-models","text":"Binocular Disparity Models Motion Parallax Models","title":"Neuroscience Models"},{"location":"pages/feature-extraction/","text":"2. Feature Extraction Computer Vision Algorithms Filters In the VisualAlgo::FeatureExtraction namespace, a set of filter classes are provided for image processing tasks: Filter : A base class with a pure virtual apply method for applying the filter to an image. GaussianFilter : A subclass of Filter that implements a Gaussian filter for image smoothing and noise reduction. It provides a constructor GaussianFilter(float sigma) to create a Gaussian filter with a specified sigma value, and overrides the apply method to perform Gaussian filtering on an image. The formula for the 2D Gaussian kernel is: g(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right) SobelFilterX and SobelFilterY : These are subclasses of Filter that implement the Sobel filter in the x and y directions respectively, used for edge detection and feature extraction tasks. The constructors SobelFilterX() and SobelFilterY() create the respective filters, and the apply method is overridden in each class to apply the corresponding Sobel filter on an image. The kernels are: Sobel_x = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix} Sobel_y = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} LoGFilter : The Laplacian of Gaussian (LoG) filter, used for edge detection and blob detection tasks. The constructor LoGFilter(float sigma) creates a LoG filter with a specified sigma value, and overrides the apply method to perform LoG filtering on an image. The Laplacian of Gaussian is defined as the second derivative of the Gaussian, so applying the LoG operation to an image corresponds to smoothing the image with a Gaussian filter and then finding the second derivative of the result. The formula for the 2D LoG kernel is: g(x, y) = \\frac{(x^2 + y^2 - 2\\sigma^2)}{2\\pi\\sigma^4} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right) Example Usage In this example, the GaussianFilter , SobelFilterX , and SobelFilterY classes are used to apply corresponding filters to an image. The filtered images are then saved for later analysis or visualization. #include \"FeatureExtraction/Filter.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::GaussianFilter gaussianFilter(0.8f); VisualAlgo::FeatureExtraction::SobelFilterX sobelXFilter; VisualAlgo::FeatureExtraction::SobelFilterY sobelYFilter; VisualAlgo::Matrix image_gaussian, image_sobel_x, image_sobel_y; image_gaussian = gaussianFilter.apply(image); image_sobel_x = sobelXFilter.apply(image); image_sobel_y = sobelYFilter.apply(image); image_gaussian.save(\"datasets/FeatureExtraction/cat_gaussian.ppm\", true); image_sobel_x.save(\"datasets/FeatureExtraction/cat_sobel_x.ppm\", true); image_sobel_y.save(\"datasets/FeatureExtraction/cat_sobel_y.ppm\", true); Gradients Class The Gradients class in the VisualAlgo::FeatureExtraction namespace is a utility class for computing the x and y gradients of an image, which are important components in various computer vision and image processing tasks such as edge detection and feature extraction. Static Functions Matrix computeXGradient(const Matrix& image) : This function computes the x-direction gradients of the image using a Sobel filter. Matrix computeYGradient(const Matrix& image) : This function computes the y-direction gradients of the image using a Sobel filter. Matrix computeGradientMagnitude(const Matrix& xGradient, const Matrix& yGradient) : This function computes the magnitude of the gradient at each pixel, defined as the square root of the sum of the squares of the x and y gradients. The output is a Matrix where each element represents the gradient magnitude at the corresponding pixel. Matrix computeGradientDirection(const Matrix& xGradient, const Matrix& yGradient, float threshold = 0.01) : This function computes the direction of the gradient at each pixel, defined as the arctangent of the ratio of the y-gradient to the x-gradient. It accepts a threshold parameter to filter out low magnitude gradients, reducing the noise. Any gradient with a magnitude less than the threshold will be set to zero in the output matrix. Example Usage In this example, the Gradients class is used to compute the x and y gradients of an image. These gradients are then saved to file and compared with the expected gradients to ensure the computations are correct. #include \"FeatureExtraction/Gradients.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::Matrix image_x_gradient, image_y_gradient; image_x_gradient = VisualAlgo::FeatureExtraction::Gradients::computeXGradient(image); image_y_gradient = VisualAlgo::FeatureExtraction::Gradients::computeYGradient(image); VisualAlgo::Matrix gradient_direction; gradient_direction = VisualAlgo::FeatureExtraction::Gradients::computeGradientDirection(image_x_gradient, image_y_gradient, 0.01); image_x_gradient.save(\"datasets/FeatureExtraction/cat_x_gradient.ppm\", true); image_y_gradient.save(\"datasets/FeatureExtraction/cat_y_gradient.ppm\", true); gradient_direction.save(\"datasets/FeatureExtraction/cat_gradient_direction.ppm\", true); In this code, computeXGradient and computeYGradient are used to calculate the x and y gradients of the loaded image respectively. The computeGradientDirection function is then used to calculate the gradient direction of the image, with a threshold of 0.01 for filtering out low magnitude gradients. The resulting gradient direction image is then saved to file for later analysis or visualization. Warning The gradient direction computation involves taking the arctangent of the ratio of the y-gradient to the x-gradient. In areas of the image where the x-gradient is near zero, this ratio can become very large or very small, leading to potential instability in the arctangent calculation. To mitigate this, we provide an option to set a threshold, under which the gradient magnitude is deemed insignificant and the direction is set to zero, reducing the influence of noise or small variations in uniform areas of the image. However, care should be taken when setting the threshold as a very high value might discard relevant information while a very low value might not effectively filter out the noise. Visual Examples Below are visual examples of the original image and the computed gradients, gradient magnitude, and gradient direction. Original Image: X-direction Gradient: Y-direction Gradient: Gradient Magnitude: Gradient Direction: Edge Non-Maximum Suppression The EdgeNonMaxSuppression class in the VisualAlgo::FeatureExtraction namespace is used for edge thinning in an image. It operates by suppressing all the non-maximum edges in the computed gradient of the image, leading to thin edges in the output. This is a key step in several edge detection algorithms such as the Canny edge detector. Class Methods Matrix apply(const Matrix &image) : This method takes as input a Matrix representing an image and applies edge non-maximum suppression to it. It first computes the x and y gradients of the image, then the gradient magnitude and direction using the Gradients class. After that, it calls the other apply method with the computed gradient magnitude and direction as arguments. Matrix apply(const Matrix &gradientMagnitude, const Matrix &gradientDirection) : This method takes as input a Matrix each representing the gradient magnitude and direction of an image. It then applies edge non-maximum suppression to it. For each pixel, it rounds the gradient direction to one of four possible directions, then compares the gradient magnitude of the current pixel with its two neighbors in the direction of the gradient. If the gradient magnitude of the current pixel is greater than both of its neighbors, it is preserved in the output; otherwise, it is suppressed. Example Usage In this example, the EdgeNonMaxSuppression class is used to apply edge non-maximum suppression to an image. The image is first loaded and normalized. The apply function of the EdgeNonMaxSuppression class is then called with the image as argument, and the resulting edge-thinned image is saved to file. #include \"FeatureExtraction/EdgeNonMaxSuppression.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::EdgeNonMaxSuppression edgeNonMaxSuppression; VisualAlgo::Matrix edge_thinned_image; edge_thinned_image = edgeNonMaxSuppression.apply(image); edge_thinned_image.save(\"datasets/FeatureExtraction/cat_edge_non_max_suppression.ppm\", true); Note The implementation assumes that the image has already been smoothed to remove noise and that appropriate gradient magnitude and direction have been computed. The edge non-maximum suppression is then used to thin out the edges in the image. Canny Edge Detection The Canny class in the VisualAlgo::FeatureExtraction namespace is an implementation of the Canny edge detection algorithm. The Canny edge detection algorithm, developed by John F. Canny in 1986, is a multi-stage algorithm used to detect a wide range of edges in images. The Canny algorithm involves several stages: Noise Reduction : Since edge detection is susceptible to noise in an image, the first step is to remove the noise. This is typically done using a Gaussian filter. Gradient Calculation : The edge in an image is the area where there is a sharp change in the color or intensity of the image. The Gradient calculation step measures this change in the x and y direction. Non-maximum Suppression : The Gradient calculation process results in thick edges. The purpose of non-maximum suppression is to convert these thick edges into thin lines. Double Threshold : Potential edges are determined by thresholding the remaining pixels based on their gradient value. This results in strong edges, weak edges, and non-edges. Edge Tracking by Hysteresis : Weak edges are pruned based on their connectivity. If a weak edge is connected to a strong edge, it is considered part of an edge. Otherwise, it is discarded. Class Members and Methods Canny(float sigma, float low_threshold, float high_threshold) : Constructor that initializes a Canny instance with the specified sigma value for the Gaussian filter, and low and high threshold values for edge detection. Matrix apply(const Matrix &image) : Applies the Canny edge detection algorithm to an input image. This function first applies Gaussian filtering to the input image for smoothing and noise reduction, then computes the gradients of the smoothed image. After that, it applies non-maximum suppression to the gradient magnitude of the image to thin the edges. Finally, it applies thresholding and edge tracking to detect the edges in the image. Matrix applyThreshold(const Matrix &image) : A private method that applies thresholding to an image based on the low and high threshold values. This is used to detect potential edges in the image. Matrix trackEdges(const Matrix &image) : A private method that tracks edges in an image using hysteresis thresholding. This is used to finalize the detected edges in the image. Example Usage In this example, the Canny class is used to apply the Canny edge detection algorithm to a cat image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Canny.hpp\" VisualAlgo::Matrix image, image_canny_expected; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); VisualAlgo::FeatureExtraction::Canny canny(1.0f, 0.1f, 0.2f); VisualAlgo::Matrix image_canny = canny.apply(image); image_canny.save(\"datasets/FeatureExtraction/cat_canny.ppm\", true); Visual Examples Below are visual examples of the original image and the computed Canny edges. Original Image: Canny Edges (sigma = 1.0, low_threshold = 0.1, high_threshold = 0.2): Harris Corner Detection The Harris class in the VisualAlgo::FeatureExtraction namespace provides an implementation of the Harris Corner Detection algorithm. This algorithm, introduced by Chris Harris and Mike Stephens in 1988, is a corner detection operator that identifies corners and edge junctions in images. It is effective due to its invariance to rotation, scale, and illumination changes. The Harris Corner Detection algorithm has several stages: Gradient Calculation : The algorithm begins by calculating the gradient images Ix and Iy. These are obtained by convolving the original image with a derivative of Gaussian filter, providing a measure of intensity change in both the x and y directions. Components of the Structure Tensor : The algorithm then calculates three images, each corresponding to the components of the structure tensor (also known as the second moment matrix) for each pixel. These images are I_x^2 , I_y^2 , and I_x \\cdot I_y , representing the gradient squared in each direction and the product of the gradients, respectively. Gaussian smoothing : Next, the images obtained from the previous step are convolved with a Gaussian filter. This smoothing process allows for the aggregation of the squared and product of gradients over a certain neighborhood, leading to the images S_{xx} , S_{yy} , and S_{xy} . Corner Response Calculation : The corner response matrix, R , is calculated for each pixel in the image using the formula R = \\det(M) - k*(\\text{trace}(M))^2 , where M is the structure tensor, or second moment matrix. M is a 2 \\times 2 matrix defined as follows: M = \\begin{bmatrix} S_{xx} & S_{xy} \\\\ S_{xy} & S_{yy} \\end{bmatrix} The determinant of M ( \\det(M) ) and the trace of M ( \\text{trace}(M) ) are computed as follows: \\det(M) = S_{xx} \\cdot S_{yy} - S_{xy} \\cdot S_{xy} \\text{trace}(M) = S_{xx} + S_{yy} The structure tensor M plays a key role in feature detection as it represents the distribution of gradients within a specific neighborhood around a point. Rather than directly comparing the gradient of a pixel with those of its neighbors, we use a Gaussian function to calculate an average gradient across an area. In essence, the structure tensor captures the underlying geometric structure in the vicinity of each pixel. It accomplishes this by portraying gradient orientations as an ellipse in the ( I_x, I_y ) plane within a specific window. Here, the determinant is directly proportional to the area of the ellipse, while the trace is equivalent to the sum of the lengths of the ellipse's major and minor axes. Presence of an edge: When an image contains an edge, the distribution of gradients forms a slender, elongated ellipse. This happens because the intensity changes consistently in one direction (along the edge) and shows little to no change in the direction perpendicular to it. The major axis of this ellipse aligns with the direction of the edge. Presence of a corner: If a corner is present, the gradients are distributed more evenly, resulting in an elliptical shape that resembles a circle. This is because a corner features significant intensity changes in multiple directions. Flat region: In a flat region of the image, where there is minimal change in intensity in any direction, the ellipse is small, signaling the absence of distinctive features. Thresholding : The final step involves applying a threshold value to the corner response matrix, R . Positions in the image that correspond to R values above the threshold are considered corners. The output is an image with highlighted positions where corners exist. Class Members and Methods Harris(float sigma, float k, float threshold) : Constructor that initializes a Harris instance with the specified sigma value for the Gaussian filter, a k value used in the formula for the response R , and a threshold value for detecting corners. Matrix apply(const Matrix &image) const : Applies the Harris Corner Detection algorith to an input image. std::vector<std::pair<int, int>> detect(const Matrix &image) : Applies the Harris Corner Detection algorithm to an input image. The apply method above actually calls this method. Example Usage In this example, the Harris class is used to apply the Harris Corner Detection algorithm to a cat image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Harris.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::Harris harris(1.0f, 0.04f, 0.2f); VisualAlgo::Matrix image_harris = harris.apply(image); image_harris.save(\"datasets/FeatureExtraction/cat_harris.ppm\", true); Visual Examples Below are visual examples of the original image and the detected corners. Original Image: Detected Corners (sigma = 1.0, k = 0.04, threshold = 0.2): Blob Detection using DoG and LoG The BlobDoG and BlobLoG classes in the VisualAlgo::FeatureExtraction namespace implement blob detection algorithms using Difference of Gaussians (DoG) and Laplacian of Gaussians (LoG) respectively. In image processing, a \"blob\" refers to a group of pixels within an image that share specific characteristics, forming a distinct region of interest. Notably, detected blobs may not always resemble what we typically visualize as a blob, since their identification relies more on algorithmic criteria rather than human visual perception. The input image is processed with a Gaussian filter (for DoG) or a Laplacian of Gaussian filter (for LoG) at different scales, generating what we refer to as \"scale-space\" - a 3D representation. Subsequently, a 3D window is used to locate the local maxima. (Yes, this comparison is not conducted across the entire scale but within a windowed range of scales.) Class Members and Methods BlobDoG(float initial_sigma, float k, float threshold, int window_size, int octaves) and BlobLoG(float initial_sigma, float k, float threshold, int window_size, int octaves) : Constructors that initialize a BlobDoG or BlobLoG instance with the specified parameters. initial_sigma : the initial standard deviation for the Gaussian filter. It determines the size of the smallest scale (or blob) that can be detected. It should be a positive value. k : the scale multiplication factor. It determines the factor by which the scale increases for each subsequent layer in an octave. It can be any real number other than 1. If k is greater than 1, the scales increase in size, and if k is less than 1, the scales decrease in size. threshold : the minimum intensity difference to consider for a point to be a local maximum in the scale-space. It should be a positive value. window_size : the size of the window used for local maxima detection in the scale-space. It needs to be an odd, positive integer and determines the size of the 3D neighborhood (x, y, and scale) within which the local maxima are searched. This helps in detecting blobs of varying sizes. octaves : the number of octaves to be used in the scale-space representation. An octave in this context represents a series of scale-space layers where the scale doubles from beginning to end. It should be a positive integer. Matrix apply(const Matrix &image) : Applies the blob detection algorithm to an input image and returns a Matrix where the blobs are highlighted. std::vector<std::tuple<int, int, float>> detect(const Matrix &image) : Returns a vector of tuples indicating the detected blobs. Each tuple contains the row and column of the detected blob and the sigma value at which the blob was detected. Example Usage In this example, the BlobDoG and BlobLoG classes are used to apply the blob detection algorithms to an image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Blob.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); VisualAlgo::FeatureExtraction::BlobDoG blobDoG(1.0f, 1.6f, 0.03f, 3, 4); VisualAlgo::Matrix image_blobs_DoG = blobDoG.apply(image); image_blobs_DoG.save(\"datasets/FeatureExtraction/cat_blobs_DoG.ppm\", true); VisualAlgo::FeatureExtraction::BlobLoG blobLoG(1.0f, 1.6f, 0.03f, 3, 4); VisualAlgo::Matrix image_blobs_LoG = blobLoG.apply(image); image_blobs_LoG.save(\"datasets/FeatureExtraction/cat_blobs_LoG.ppm\", true); Visual Examples Below are visual examples of the original image and the detected blobs using DoG and LoG. Original Image: Blobs using DoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1): Blobs using LoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1): SIFT SURF ORB HOG Neuroscience Models Simple and Complex Cell Models","title":"2. Feature Extraction"},{"location":"pages/feature-extraction/#2-feature-extraction","text":"","title":"2. Feature Extraction"},{"location":"pages/feature-extraction/#computer-vision-algorithms","text":"","title":"Computer Vision Algorithms"},{"location":"pages/feature-extraction/#filters","text":"In the VisualAlgo::FeatureExtraction namespace, a set of filter classes are provided for image processing tasks: Filter : A base class with a pure virtual apply method for applying the filter to an image. GaussianFilter : A subclass of Filter that implements a Gaussian filter for image smoothing and noise reduction. It provides a constructor GaussianFilter(float sigma) to create a Gaussian filter with a specified sigma value, and overrides the apply method to perform Gaussian filtering on an image. The formula for the 2D Gaussian kernel is: g(x, y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right) SobelFilterX and SobelFilterY : These are subclasses of Filter that implement the Sobel filter in the x and y directions respectively, used for edge detection and feature extraction tasks. The constructors SobelFilterX() and SobelFilterY() create the respective filters, and the apply method is overridden in each class to apply the corresponding Sobel filter on an image. The kernels are: Sobel_x = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix} Sobel_y = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} LoGFilter : The Laplacian of Gaussian (LoG) filter, used for edge detection and blob detection tasks. The constructor LoGFilter(float sigma) creates a LoG filter with a specified sigma value, and overrides the apply method to perform LoG filtering on an image. The Laplacian of Gaussian is defined as the second derivative of the Gaussian, so applying the LoG operation to an image corresponds to smoothing the image with a Gaussian filter and then finding the second derivative of the result. The formula for the 2D LoG kernel is: g(x, y) = \\frac{(x^2 + y^2 - 2\\sigma^2)}{2\\pi\\sigma^4} \\exp\\left(-\\frac{x^2 + y^2}{2\\sigma^2}\\right)","title":"Filters"},{"location":"pages/feature-extraction/#example-usage","text":"In this example, the GaussianFilter , SobelFilterX , and SobelFilterY classes are used to apply corresponding filters to an image. The filtered images are then saved for later analysis or visualization. #include \"FeatureExtraction/Filter.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::GaussianFilter gaussianFilter(0.8f); VisualAlgo::FeatureExtraction::SobelFilterX sobelXFilter; VisualAlgo::FeatureExtraction::SobelFilterY sobelYFilter; VisualAlgo::Matrix image_gaussian, image_sobel_x, image_sobel_y; image_gaussian = gaussianFilter.apply(image); image_sobel_x = sobelXFilter.apply(image); image_sobel_y = sobelYFilter.apply(image); image_gaussian.save(\"datasets/FeatureExtraction/cat_gaussian.ppm\", true); image_sobel_x.save(\"datasets/FeatureExtraction/cat_sobel_x.ppm\", true); image_sobel_y.save(\"datasets/FeatureExtraction/cat_sobel_y.ppm\", true);","title":"Example Usage"},{"location":"pages/feature-extraction/#gradients-class","text":"The Gradients class in the VisualAlgo::FeatureExtraction namespace is a utility class for computing the x and y gradients of an image, which are important components in various computer vision and image processing tasks such as edge detection and feature extraction.","title":"Gradients Class"},{"location":"pages/feature-extraction/#static-functions","text":"Matrix computeXGradient(const Matrix& image) : This function computes the x-direction gradients of the image using a Sobel filter. Matrix computeYGradient(const Matrix& image) : This function computes the y-direction gradients of the image using a Sobel filter. Matrix computeGradientMagnitude(const Matrix& xGradient, const Matrix& yGradient) : This function computes the magnitude of the gradient at each pixel, defined as the square root of the sum of the squares of the x and y gradients. The output is a Matrix where each element represents the gradient magnitude at the corresponding pixel. Matrix computeGradientDirection(const Matrix& xGradient, const Matrix& yGradient, float threshold = 0.01) : This function computes the direction of the gradient at each pixel, defined as the arctangent of the ratio of the y-gradient to the x-gradient. It accepts a threshold parameter to filter out low magnitude gradients, reducing the noise. Any gradient with a magnitude less than the threshold will be set to zero in the output matrix.","title":"Static Functions"},{"location":"pages/feature-extraction/#example-usage_1","text":"In this example, the Gradients class is used to compute the x and y gradients of an image. These gradients are then saved to file and compared with the expected gradients to ensure the computations are correct. #include \"FeatureExtraction/Gradients.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::Matrix image_x_gradient, image_y_gradient; image_x_gradient = VisualAlgo::FeatureExtraction::Gradients::computeXGradient(image); image_y_gradient = VisualAlgo::FeatureExtraction::Gradients::computeYGradient(image); VisualAlgo::Matrix gradient_direction; gradient_direction = VisualAlgo::FeatureExtraction::Gradients::computeGradientDirection(image_x_gradient, image_y_gradient, 0.01); image_x_gradient.save(\"datasets/FeatureExtraction/cat_x_gradient.ppm\", true); image_y_gradient.save(\"datasets/FeatureExtraction/cat_y_gradient.ppm\", true); gradient_direction.save(\"datasets/FeatureExtraction/cat_gradient_direction.ppm\", true); In this code, computeXGradient and computeYGradient are used to calculate the x and y gradients of the loaded image respectively. The computeGradientDirection function is then used to calculate the gradient direction of the image, with a threshold of 0.01 for filtering out low magnitude gradients. The resulting gradient direction image is then saved to file for later analysis or visualization.","title":"Example Usage"},{"location":"pages/feature-extraction/#warning","text":"The gradient direction computation involves taking the arctangent of the ratio of the y-gradient to the x-gradient. In areas of the image where the x-gradient is near zero, this ratio can become very large or very small, leading to potential instability in the arctangent calculation. To mitigate this, we provide an option to set a threshold, under which the gradient magnitude is deemed insignificant and the direction is set to zero, reducing the influence of noise or small variations in uniform areas of the image. However, care should be taken when setting the threshold as a very high value might discard relevant information while a very low value might not effectively filter out the noise.","title":"Warning"},{"location":"pages/feature-extraction/#visual-examples","text":"Below are visual examples of the original image and the computed gradients, gradient magnitude, and gradient direction. Original Image: X-direction Gradient: Y-direction Gradient: Gradient Magnitude: Gradient Direction:","title":"Visual Examples"},{"location":"pages/feature-extraction/#edge-non-maximum-suppression","text":"The EdgeNonMaxSuppression class in the VisualAlgo::FeatureExtraction namespace is used for edge thinning in an image. It operates by suppressing all the non-maximum edges in the computed gradient of the image, leading to thin edges in the output. This is a key step in several edge detection algorithms such as the Canny edge detector.","title":"Edge Non-Maximum Suppression"},{"location":"pages/feature-extraction/#class-methods","text":"Matrix apply(const Matrix &image) : This method takes as input a Matrix representing an image and applies edge non-maximum suppression to it. It first computes the x and y gradients of the image, then the gradient magnitude and direction using the Gradients class. After that, it calls the other apply method with the computed gradient magnitude and direction as arguments. Matrix apply(const Matrix &gradientMagnitude, const Matrix &gradientDirection) : This method takes as input a Matrix each representing the gradient magnitude and direction of an image. It then applies edge non-maximum suppression to it. For each pixel, it rounds the gradient direction to one of four possible directions, then compares the gradient magnitude of the current pixel with its two neighbors in the direction of the gradient. If the gradient magnitude of the current pixel is greater than both of its neighbors, it is preserved in the output; otherwise, it is suppressed.","title":"Class Methods"},{"location":"pages/feature-extraction/#example-usage_2","text":"In this example, the EdgeNonMaxSuppression class is used to apply edge non-maximum suppression to an image. The image is first loaded and normalized. The apply function of the EdgeNonMaxSuppression class is then called with the image as argument, and the resulting edge-thinned image is saved to file. #include \"FeatureExtraction/EdgeNonMaxSuppression.hpp\" #include \"helpers/Matrix.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::EdgeNonMaxSuppression edgeNonMaxSuppression; VisualAlgo::Matrix edge_thinned_image; edge_thinned_image = edgeNonMaxSuppression.apply(image); edge_thinned_image.save(\"datasets/FeatureExtraction/cat_edge_non_max_suppression.ppm\", true);","title":"Example Usage"},{"location":"pages/feature-extraction/#note","text":"The implementation assumes that the image has already been smoothed to remove noise and that appropriate gradient magnitude and direction have been computed. The edge non-maximum suppression is then used to thin out the edges in the image.","title":"Note"},{"location":"pages/feature-extraction/#canny-edge-detection","text":"The Canny class in the VisualAlgo::FeatureExtraction namespace is an implementation of the Canny edge detection algorithm. The Canny edge detection algorithm, developed by John F. Canny in 1986, is a multi-stage algorithm used to detect a wide range of edges in images. The Canny algorithm involves several stages: Noise Reduction : Since edge detection is susceptible to noise in an image, the first step is to remove the noise. This is typically done using a Gaussian filter. Gradient Calculation : The edge in an image is the area where there is a sharp change in the color or intensity of the image. The Gradient calculation step measures this change in the x and y direction. Non-maximum Suppression : The Gradient calculation process results in thick edges. The purpose of non-maximum suppression is to convert these thick edges into thin lines. Double Threshold : Potential edges are determined by thresholding the remaining pixels based on their gradient value. This results in strong edges, weak edges, and non-edges. Edge Tracking by Hysteresis : Weak edges are pruned based on their connectivity. If a weak edge is connected to a strong edge, it is considered part of an edge. Otherwise, it is discarded.","title":"Canny Edge Detection"},{"location":"pages/feature-extraction/#class-members-and-methods","text":"Canny(float sigma, float low_threshold, float high_threshold) : Constructor that initializes a Canny instance with the specified sigma value for the Gaussian filter, and low and high threshold values for edge detection. Matrix apply(const Matrix &image) : Applies the Canny edge detection algorithm to an input image. This function first applies Gaussian filtering to the input image for smoothing and noise reduction, then computes the gradients of the smoothed image. After that, it applies non-maximum suppression to the gradient magnitude of the image to thin the edges. Finally, it applies thresholding and edge tracking to detect the edges in the image. Matrix applyThreshold(const Matrix &image) : A private method that applies thresholding to an image based on the low and high threshold values. This is used to detect potential edges in the image. Matrix trackEdges(const Matrix &image) : A private method that tracks edges in an image using hysteresis thresholding. This is used to finalize the detected edges in the image.","title":"Class Members and Methods"},{"location":"pages/feature-extraction/#example-usage_3","text":"In this example, the Canny class is used to apply the Canny edge detection algorithm to a cat image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Canny.hpp\" VisualAlgo::Matrix image, image_canny_expected; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); VisualAlgo::FeatureExtraction::Canny canny(1.0f, 0.1f, 0.2f); VisualAlgo::Matrix image_canny = canny.apply(image); image_canny.save(\"datasets/FeatureExtraction/cat_canny.ppm\", true);","title":"Example Usage"},{"location":"pages/feature-extraction/#visual-examples_1","text":"Below are visual examples of the original image and the computed Canny edges. Original Image: Canny Edges (sigma = 1.0, low_threshold = 0.1, high_threshold = 0.2):","title":"Visual Examples"},{"location":"pages/feature-extraction/#harris-corner-detection","text":"The Harris class in the VisualAlgo::FeatureExtraction namespace provides an implementation of the Harris Corner Detection algorithm. This algorithm, introduced by Chris Harris and Mike Stephens in 1988, is a corner detection operator that identifies corners and edge junctions in images. It is effective due to its invariance to rotation, scale, and illumination changes. The Harris Corner Detection algorithm has several stages: Gradient Calculation : The algorithm begins by calculating the gradient images Ix and Iy. These are obtained by convolving the original image with a derivative of Gaussian filter, providing a measure of intensity change in both the x and y directions. Components of the Structure Tensor : The algorithm then calculates three images, each corresponding to the components of the structure tensor (also known as the second moment matrix) for each pixel. These images are I_x^2 , I_y^2 , and I_x \\cdot I_y , representing the gradient squared in each direction and the product of the gradients, respectively. Gaussian smoothing : Next, the images obtained from the previous step are convolved with a Gaussian filter. This smoothing process allows for the aggregation of the squared and product of gradients over a certain neighborhood, leading to the images S_{xx} , S_{yy} , and S_{xy} . Corner Response Calculation : The corner response matrix, R , is calculated for each pixel in the image using the formula R = \\det(M) - k*(\\text{trace}(M))^2 , where M is the structure tensor, or second moment matrix. M is a 2 \\times 2 matrix defined as follows: M = \\begin{bmatrix} S_{xx} & S_{xy} \\\\ S_{xy} & S_{yy} \\end{bmatrix} The determinant of M ( \\det(M) ) and the trace of M ( \\text{trace}(M) ) are computed as follows: \\det(M) = S_{xx} \\cdot S_{yy} - S_{xy} \\cdot S_{xy} \\text{trace}(M) = S_{xx} + S_{yy} The structure tensor M plays a key role in feature detection as it represents the distribution of gradients within a specific neighborhood around a point. Rather than directly comparing the gradient of a pixel with those of its neighbors, we use a Gaussian function to calculate an average gradient across an area. In essence, the structure tensor captures the underlying geometric structure in the vicinity of each pixel. It accomplishes this by portraying gradient orientations as an ellipse in the ( I_x, I_y ) plane within a specific window. Here, the determinant is directly proportional to the area of the ellipse, while the trace is equivalent to the sum of the lengths of the ellipse's major and minor axes. Presence of an edge: When an image contains an edge, the distribution of gradients forms a slender, elongated ellipse. This happens because the intensity changes consistently in one direction (along the edge) and shows little to no change in the direction perpendicular to it. The major axis of this ellipse aligns with the direction of the edge. Presence of a corner: If a corner is present, the gradients are distributed more evenly, resulting in an elliptical shape that resembles a circle. This is because a corner features significant intensity changes in multiple directions. Flat region: In a flat region of the image, where there is minimal change in intensity in any direction, the ellipse is small, signaling the absence of distinctive features. Thresholding : The final step involves applying a threshold value to the corner response matrix, R . Positions in the image that correspond to R values above the threshold are considered corners. The output is an image with highlighted positions where corners exist.","title":"Harris Corner Detection"},{"location":"pages/feature-extraction/#class-members-and-methods_1","text":"Harris(float sigma, float k, float threshold) : Constructor that initializes a Harris instance with the specified sigma value for the Gaussian filter, a k value used in the formula for the response R , and a threshold value for detecting corners. Matrix apply(const Matrix &image) const : Applies the Harris Corner Detection algorith to an input image. std::vector<std::pair<int, int>> detect(const Matrix &image) : Applies the Harris Corner Detection algorithm to an input image. The apply method above actually calls this method.","title":"Class Members and Methods"},{"location":"pages/feature-extraction/#example-usage_4","text":"In this example, the Harris class is used to apply the Harris Corner Detection algorithm to a cat image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Harris.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); image.normalize(); VisualAlgo::FeatureExtraction::Harris harris(1.0f, 0.04f, 0.2f); VisualAlgo::Matrix image_harris = harris.apply(image); image_harris.save(\"datasets/FeatureExtraction/cat_harris.ppm\", true);","title":"Example Usage"},{"location":"pages/feature-extraction/#visual-examples_2","text":"Below are visual examples of the original image and the detected corners. Original Image: Detected Corners (sigma = 1.0, k = 0.04, threshold = 0.2):","title":"Visual Examples"},{"location":"pages/feature-extraction/#blob-detection-using-dog-and-log","text":"The BlobDoG and BlobLoG classes in the VisualAlgo::FeatureExtraction namespace implement blob detection algorithms using Difference of Gaussians (DoG) and Laplacian of Gaussians (LoG) respectively. In image processing, a \"blob\" refers to a group of pixels within an image that share specific characteristics, forming a distinct region of interest. Notably, detected blobs may not always resemble what we typically visualize as a blob, since their identification relies more on algorithmic criteria rather than human visual perception. The input image is processed with a Gaussian filter (for DoG) or a Laplacian of Gaussian filter (for LoG) at different scales, generating what we refer to as \"scale-space\" - a 3D representation. Subsequently, a 3D window is used to locate the local maxima. (Yes, this comparison is not conducted across the entire scale but within a windowed range of scales.)","title":"Blob Detection using DoG and LoG"},{"location":"pages/feature-extraction/#class-members-and-methods_2","text":"BlobDoG(float initial_sigma, float k, float threshold, int window_size, int octaves) and BlobLoG(float initial_sigma, float k, float threshold, int window_size, int octaves) : Constructors that initialize a BlobDoG or BlobLoG instance with the specified parameters. initial_sigma : the initial standard deviation for the Gaussian filter. It determines the size of the smallest scale (or blob) that can be detected. It should be a positive value. k : the scale multiplication factor. It determines the factor by which the scale increases for each subsequent layer in an octave. It can be any real number other than 1. If k is greater than 1, the scales increase in size, and if k is less than 1, the scales decrease in size. threshold : the minimum intensity difference to consider for a point to be a local maximum in the scale-space. It should be a positive value. window_size : the size of the window used for local maxima detection in the scale-space. It needs to be an odd, positive integer and determines the size of the 3D neighborhood (x, y, and scale) within which the local maxima are searched. This helps in detecting blobs of varying sizes. octaves : the number of octaves to be used in the scale-space representation. An octave in this context represents a series of scale-space layers where the scale doubles from beginning to end. It should be a positive integer. Matrix apply(const Matrix &image) : Applies the blob detection algorithm to an input image and returns a Matrix where the blobs are highlighted. std::vector<std::tuple<int, int, float>> detect(const Matrix &image) : Returns a vector of tuples indicating the detected blobs. Each tuple contains the row and column of the detected blob and the sigma value at which the blob was detected.","title":"Class Members and Methods"},{"location":"pages/feature-extraction/#example-usage_5","text":"In this example, the BlobDoG and BlobLoG classes are used to apply the blob detection algorithms to an image. #include \"helpers/Matrix.hpp\" #include \"FeatureExtraction/Blob.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/FeatureExtraction/cat_resized.ppm\"); VisualAlgo::FeatureExtraction::BlobDoG blobDoG(1.0f, 1.6f, 0.03f, 3, 4); VisualAlgo::Matrix image_blobs_DoG = blobDoG.apply(image); image_blobs_DoG.save(\"datasets/FeatureExtraction/cat_blobs_DoG.ppm\", true); VisualAlgo::FeatureExtraction::BlobLoG blobLoG(1.0f, 1.6f, 0.03f, 3, 4); VisualAlgo::Matrix image_blobs_LoG = blobLoG.apply(image); image_blobs_LoG.save(\"datasets/FeatureExtraction/cat_blobs_LoG.ppm\", true);","title":"Example Usage"},{"location":"pages/feature-extraction/#visual-examples_3","text":"Below are visual examples of the original image and the detected blobs using DoG and LoG. Original Image: Blobs using DoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1): Blobs using LoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1):","title":"Visual Examples"},{"location":"pages/feature-extraction/#sift","text":"","title":"SIFT"},{"location":"pages/feature-extraction/#surf","text":"","title":"SURF"},{"location":"pages/feature-extraction/#orb","text":"","title":"ORB"},{"location":"pages/feature-extraction/#hog","text":"","title":"HOG"},{"location":"pages/feature-extraction/#neuroscience-models","text":"","title":"Neuroscience Models"},{"location":"pages/feature-extraction/#simple-and-complex-cell-models","text":"","title":"Simple and Complex Cell Models"},{"location":"pages/getting-started/","text":"Getting Started Prerequisites You will need a compiler that supports C++17 to build and run this project. No external libraries are required at this moment. Instructions Follow these steps to get a copy of the project and build the library on your local machine: Clone the project to your local machine: git clone git@github.com:tonyfu97/VisualAlgo.git Navigate to the project directory: cd VisualAlgo Compile the project to generate the library: make The command will create a file named libVisualAlgo.a in the project directory. This is a static library that can be linked to your C++ projects. To use the libVisualAlgo.a library in your C++ code, include the necessary header files in your source code. When compiling your code, link against libVisualAlgo.a . Here is an example of how to do this: #include \"path_to_VisualAlgo/Interpolate.hpp\" #include \"path_to_VisualAlgo/Transform.hpp\" // other includes as needed And an example compilation command might look like this: g++ -std=c++17 your_code.cpp -L. -lVisualAlgo -o your_program Documentation The detailed documentation of each class and method is provided in the source code. Here is the documentation link for the Matrix class: Matrix documentation Note Please be aware that this library currently supports only grayscale images. All algorithms and methods have been unit tested against \"ground-truth\" images generated with Python's CV libraries such as Skimage. Despite testing, there might still be bugs. If you discover any bugs or have suggestions for improvements, feel free to raise an issue or submit a pull request.","title":"Getting Started"},{"location":"pages/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"pages/getting-started/#prerequisites","text":"You will need a compiler that supports C++17 to build and run this project. No external libraries are required at this moment.","title":"Prerequisites"},{"location":"pages/getting-started/#instructions","text":"Follow these steps to get a copy of the project and build the library on your local machine: Clone the project to your local machine: git clone git@github.com:tonyfu97/VisualAlgo.git Navigate to the project directory: cd VisualAlgo Compile the project to generate the library: make The command will create a file named libVisualAlgo.a in the project directory. This is a static library that can be linked to your C++ projects. To use the libVisualAlgo.a library in your C++ code, include the necessary header files in your source code. When compiling your code, link against libVisualAlgo.a . Here is an example of how to do this: #include \"path_to_VisualAlgo/Interpolate.hpp\" #include \"path_to_VisualAlgo/Transform.hpp\" // other includes as needed And an example compilation command might look like this: g++ -std=c++17 your_code.cpp -L. -lVisualAlgo -o your_program","title":"Instructions"},{"location":"pages/getting-started/#documentation","text":"The detailed documentation of each class and method is provided in the source code. Here is the documentation link for the Matrix class: Matrix documentation","title":"Documentation"},{"location":"pages/getting-started/#note","text":"Please be aware that this library currently supports only grayscale images. All algorithms and methods have been unit tested against \"ground-truth\" images generated with Python's CV libraries such as Skimage. Despite testing, there might still be bugs. If you discover any bugs or have suggestions for improvements, feel free to raise an issue or submit a pull request.","title":"Note"},{"location":"pages/image-preprocessing-and-enhancement/","text":"1. Image Pre-processing and Enhancement Computer Vision Algorithms Interpolate The Interpolate class in the VisualAlgo::ImagePreprocessingAndEnhancement namespace provides different interpolation methods for scaling and enhancing image resolution. Interpolate is a method of estimating values between two points in an image or a series of data points. The Interpolate class provides three common types of interpolation: nearest neighbor, bilinear, and bicubic. Nearest Neighbor : This is the simplest interpolation method where the value of an unknown pixel is assumed to be equal to the value of the nearest pixel in the input image. More precisely, we consider the pixel in the input image that is to the left and/or above (according to the decimal part of the coordinates), in other words, the floor of the coordinate value is used to determine the nearest neighbor for interpolation. Bilinear : This method takes the weighted average of the four nearest pixels to estimate the value of an unknown pixel. The weights are determined by the distance of the unknown pixel from these known pixels. Bicubic : A slightly more sophisticated method. It estimates the value of an unknown pixel by taking the weighted average of the sixteen nearest pixels. The weights are determined by the distance of the unknown pixel from these known pixels and are calculated using a cubic function. Bicubic interpolation uses cubic polynomials to estimate pixel intensities near the pixel of interest P(t) . Particularly, this implementation uses the Catmull-Rom splines, which pass through two control points, P_1 and P_2 . These points surround the point of interest, while P_0 and P_3 are used to determine the slope. The Catmull-Rom spline is designed so that the interpolated pixel value is a weighted sum of the four pixels surrounding it. Here's the 1D version with t representing the point of interest: P(t) = \\sum_{i=0}^{3} P_i Q_i(t) where P_i are the control points, and Q_i(t) are the blending functions for the Catmull-Rom spline: Q_0(t) = -\\frac{1}{2}t^3 + t^2 - \\frac{1}{2}t \\\\ Q_1(t) = \\frac{3}{2}t^3 - 2.5t^2 + 1 \\\\ Q_2(t) = -\\frac{3}{2}t^3 + 2t^2 + \\frac{1}{2}t \\\\ Q_3(t) = \\frac{1}{2}t^3 - \\frac{1}{2}t^2 Substituting Q_i(t) into the original equation and simplifying yields: P(t) = 0.5 \\times [ (2 P_1) + (-P_0 + P_2) t + (2P_0 - 5 P_1 + 4 P_2 - P_3) t^2 + (-P_0 + 3 P_1 - 3 P_2 + P_3) t^3 ] This equation corresponds to the cubicInterpolation function in the provided code. Class Members and Methods enum class InterpolationType : Enumeration that provides a way to select the type of interpolation method. This can be NEAREST , BILINEAR , or BICUBIC . to_string(InterpolationType type) : A function that takes an InterpolationType and returns its string representation. nearest(const Matrix &image, float x, float y) : A static method that performs nearest neighbor interpolation on the given image at the specified coordinates (x, y) . bilinear(const Matrix &image, float x, float y) : A static method that performs bilinear interpolation on the given image at the specified coordinates (x, y) . bicubic(const Matrix &image, float x, float y) : A static method that performs bicubic interpolation on the given image at the specified coordinates (x, y) . interpolate(const Matrix &image, float x, float y, InterpolationType type, float default_value=0.0f) : A static method that interpolates the given image at the specified coordinates (x, y) using the specified interpolation type. If (x, y) is outside of the image, then it will return default_value. interpolate(const Matrix &image, float scale, InterpolationType type) : A static method that scales the given image by the specified scale factor using the specified interpolation type. interpolate(const Matrix &image, int rows, int cols, InterpolationType type) : A static method that resizes the given image to the specified number of rows and columns using the specified interpolation type. Example Usage In this example, the Interpolate class is used to scale an image using bicubic interpolation. #include \"helpers/Matrix.hpp\" #include \"ImagePreprocessingAndEnhancement/Interpolate.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/ImagePreprocessingAndEnhancement/cat_resized.ppm\"); VisualAlgo::Matrix image_scaled = VisualAlgo::ImagePreprocessingAndEnhancement::Interpolate::interpolate(image, 2.0f, VisualAlgo::ImagePreprocessingAndEnhancement::InterpolationType::BICUBIC); image_scaled.save(\"datasets/ImagePreprocessingAndEnhancement/cat_scaled.ppm\", true); Visual Examples Below are visual examples of the original image and the image after bicubic interpolation. Original Image: Scaled Image (scale = 0.5, method = NEAREST): Scaled Image (scale = 0.5, method = BILINEAR): Scaled Image (scale = 0.5, method = BICUBIC): Note on Image Types: These interpolation methods are designed to work with grayscale images. Color images are typically represented as multi-channel images (e.g., Red-Green-Blue), and these methods do not directly apply. Note on Simplifications and Differences from Other Libraries: While these interpolation methods implement the fundamental logic of nearest neighbor, bilinear, and bicubic interpolation, there are several simplifications in the implementation. These include: Edge Handling : In the case where an interpolation point falls near the edge or outside the given image, our implementation clamps the coordinates to the image boundary. This approach is straightforward but not always ideal. Other libraries may offer more advanced edge-handling strategies, such as padding, mirroring, or extrapolation. Optimization : The bicubic interpolation method, in particular, can be further optimized. Our implementation performs the calculations in a straightforward manner, which could be computationally heavy for large images. Other libraries may use optimized routines or hardware acceleration to speed up these operations. Image Transformation Methods in VisualAlgo::ImagePreprocessingAndEnhancement::Transform The Transform class implements various image transformation operations, each using an associated 3x3 transformation matrix. These operations include translation, scaling, rotation, and shear. Translation : static Matrix translate(const Matrix &image, int dx, int dy, InterpolationType method = InterpolationType::NEAREST) Moves an image by dx units in the x-direction and dy units in the y-direction. The translation matrix is: \\begin{pmatrix} 1 & 0 & dx \\\\ 0 & 1 & dy \\\\ 0 & 0 & 1 \\end{pmatrix} Scaling : static Matrix scale(const Matrix &image, float sx, float sy, InterpolationType method = InterpolationType::NEAREST) Resizes an image by a factor of sx in the x-direction and sy in the y-direction. The scaling matrix is: \\begin{pmatrix} sx & 0 & 0 \\\\ 0 & sy & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} Rotation : static Matrix rotate(const Matrix &image, float angle, InterpolationType method = InterpolationType::NEAREST) Rotates an image by a specified angle (in radians). The rotation matrix is: \\begin{pmatrix} cos(\\theta) & -sin(\\theta) & 0 \\\\ sin(\\theta) & cos(\\theta) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} Shear : static Matrix shear(const Matrix &image, float kx, float ky, InterpolationType method = InterpolationType::NEAREST) Applies a shear transformation with the specified shear factors kx and ky in the x and y directions respectively. The shear matrix is: \\begin{pmatrix} 1 & kx & 0 \\\\ ky & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} The shearing factors kx and ky cannot both be equal to 1. This condition would result in a singular transformation matrix, and the image would collapse into a line. Affine : static Matrix affine(const Matrix &image, const Matrix &transform_matrix, InterpolationType method = InterpolationType::NEAREST) The above transformations call this method to apply the appropriate transformations. The affine transformation matrix is: \\begin{pmatrix} a & b & tx \\\\ c & d & ty \\\\ 0 & 0 & 1 \\end{pmatrix} Where a , b , c , and d represent scaling, rotation, and shear transformations, and tx and ty represent translations in the x and y directions, respectively. Perspective : static Matrix perspective(const Matrix &image, const Matrix &transform_matrix, InterpolationType method = InterpolationType::NEAREST) Perspective transformation is more general than the affine transformation due to the presence of non-zero values in the third column of the transformation matrix for perspective transformations. These values (denoted as h and i ) result in a transformation equivalent to shearing along the x and y-axes. \\begin{pmatrix} a & b & tx \\\\ c & d & ty \\\\ h & i & 1 \\end{pmatrix} Example Usage In this example, we use the Transform class to apply various transformations on an image, such as translation, scaling, rotation, shearing, and perspective transformation. #include \"TestHarness.h\" #include \"ImagePreprocessingAndEnhancement/Transform.hpp\" #include \"helpers/Matrix.hpp\" #include <string> #include <cmath> VisualAlgo::Matrix image; image.load(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_resized.ppm\"); // Translate the image VisualAlgo::Matrix image_translated = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::translate(image, 100, 50); image_translated.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_translated.ppm\", true); // Scale the image VisualAlgo::Matrix image_scaled = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::scale(image, 2, 0.5, VisualAlgo::ImagePreprocessingAndEnhancement::InterpolationType::NEAREST); image_scaled.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_scaled.ppm\", true); // Rotate the image VisualAlgo::Matrix image_rotated = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::rotate(image, M_PI / 2); image_rotated.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_rotated.ppm\", true); // Shear the image VisualAlgo::Matrix image_sheared = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::shear(image, 0, 0.5); image_sheared.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_sheared.ppm\", true); // Apply perspective transformation on the image VisualAlgo::Matrix perspective_matrix = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::scale(0.5, 0.5); perspective_matrix.set(2, 0, 0.01); perspective_matrix.set(2, 1, 0.01); VisualAlgo::Matrix image_perspective_transformed = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::perspective(image, perspective_matrix); image_perspective_transformed.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_perspective.ppm\", true); Visual Examples Here are the results from the above code: Translation Scaling Rotation Shearing Perspective Histogram Equalization Neuroscience Models","title":"1. Image Pre-processing and Enhancement"},{"location":"pages/image-preprocessing-and-enhancement/#1-image-pre-processing-and-enhancement","text":"","title":"1. Image Pre-processing and Enhancement"},{"location":"pages/image-preprocessing-and-enhancement/#computer-vision-algorithms","text":"","title":"Computer Vision Algorithms"},{"location":"pages/image-preprocessing-and-enhancement/#interpolate","text":"The Interpolate class in the VisualAlgo::ImagePreprocessingAndEnhancement namespace provides different interpolation methods for scaling and enhancing image resolution. Interpolate is a method of estimating values between two points in an image or a series of data points. The Interpolate class provides three common types of interpolation: nearest neighbor, bilinear, and bicubic. Nearest Neighbor : This is the simplest interpolation method where the value of an unknown pixel is assumed to be equal to the value of the nearest pixel in the input image. More precisely, we consider the pixel in the input image that is to the left and/or above (according to the decimal part of the coordinates), in other words, the floor of the coordinate value is used to determine the nearest neighbor for interpolation. Bilinear : This method takes the weighted average of the four nearest pixels to estimate the value of an unknown pixel. The weights are determined by the distance of the unknown pixel from these known pixels. Bicubic : A slightly more sophisticated method. It estimates the value of an unknown pixel by taking the weighted average of the sixteen nearest pixels. The weights are determined by the distance of the unknown pixel from these known pixels and are calculated using a cubic function. Bicubic interpolation uses cubic polynomials to estimate pixel intensities near the pixel of interest P(t) . Particularly, this implementation uses the Catmull-Rom splines, which pass through two control points, P_1 and P_2 . These points surround the point of interest, while P_0 and P_3 are used to determine the slope. The Catmull-Rom spline is designed so that the interpolated pixel value is a weighted sum of the four pixels surrounding it. Here's the 1D version with t representing the point of interest: P(t) = \\sum_{i=0}^{3} P_i Q_i(t) where P_i are the control points, and Q_i(t) are the blending functions for the Catmull-Rom spline: Q_0(t) = -\\frac{1}{2}t^3 + t^2 - \\frac{1}{2}t \\\\ Q_1(t) = \\frac{3}{2}t^3 - 2.5t^2 + 1 \\\\ Q_2(t) = -\\frac{3}{2}t^3 + 2t^2 + \\frac{1}{2}t \\\\ Q_3(t) = \\frac{1}{2}t^3 - \\frac{1}{2}t^2 Substituting Q_i(t) into the original equation and simplifying yields: P(t) = 0.5 \\times [ (2 P_1) + (-P_0 + P_2) t + (2P_0 - 5 P_1 + 4 P_2 - P_3) t^2 + (-P_0 + 3 P_1 - 3 P_2 + P_3) t^3 ] This equation corresponds to the cubicInterpolation function in the provided code.","title":"Interpolate"},{"location":"pages/image-preprocessing-and-enhancement/#class-members-and-methods","text":"enum class InterpolationType : Enumeration that provides a way to select the type of interpolation method. This can be NEAREST , BILINEAR , or BICUBIC . to_string(InterpolationType type) : A function that takes an InterpolationType and returns its string representation. nearest(const Matrix &image, float x, float y) : A static method that performs nearest neighbor interpolation on the given image at the specified coordinates (x, y) . bilinear(const Matrix &image, float x, float y) : A static method that performs bilinear interpolation on the given image at the specified coordinates (x, y) . bicubic(const Matrix &image, float x, float y) : A static method that performs bicubic interpolation on the given image at the specified coordinates (x, y) . interpolate(const Matrix &image, float x, float y, InterpolationType type, float default_value=0.0f) : A static method that interpolates the given image at the specified coordinates (x, y) using the specified interpolation type. If (x, y) is outside of the image, then it will return default_value. interpolate(const Matrix &image, float scale, InterpolationType type) : A static method that scales the given image by the specified scale factor using the specified interpolation type. interpolate(const Matrix &image, int rows, int cols, InterpolationType type) : A static method that resizes the given image to the specified number of rows and columns using the specified interpolation type.","title":"Class Members and Methods"},{"location":"pages/image-preprocessing-and-enhancement/#example-usage","text":"In this example, the Interpolate class is used to scale an image using bicubic interpolation. #include \"helpers/Matrix.hpp\" #include \"ImagePreprocessingAndEnhancement/Interpolate.hpp\" VisualAlgo::Matrix image; image.load(\"datasets/ImagePreprocessingAndEnhancement/cat_resized.ppm\"); VisualAlgo::Matrix image_scaled = VisualAlgo::ImagePreprocessingAndEnhancement::Interpolate::interpolate(image, 2.0f, VisualAlgo::ImagePreprocessingAndEnhancement::InterpolationType::BICUBIC); image_scaled.save(\"datasets/ImagePreprocessingAndEnhancement/cat_scaled.ppm\", true);","title":"Example Usage"},{"location":"pages/image-preprocessing-and-enhancement/#visual-examples","text":"Below are visual examples of the original image and the image after bicubic interpolation. Original Image: Scaled Image (scale = 0.5, method = NEAREST): Scaled Image (scale = 0.5, method = BILINEAR): Scaled Image (scale = 0.5, method = BICUBIC):","title":"Visual Examples"},{"location":"pages/image-preprocessing-and-enhancement/#note-on-image-types","text":"These interpolation methods are designed to work with grayscale images. Color images are typically represented as multi-channel images (e.g., Red-Green-Blue), and these methods do not directly apply.","title":"Note on Image Types:"},{"location":"pages/image-preprocessing-and-enhancement/#note-on-simplifications-and-differences-from-other-libraries","text":"While these interpolation methods implement the fundamental logic of nearest neighbor, bilinear, and bicubic interpolation, there are several simplifications in the implementation. These include: Edge Handling : In the case where an interpolation point falls near the edge or outside the given image, our implementation clamps the coordinates to the image boundary. This approach is straightforward but not always ideal. Other libraries may offer more advanced edge-handling strategies, such as padding, mirroring, or extrapolation. Optimization : The bicubic interpolation method, in particular, can be further optimized. Our implementation performs the calculations in a straightforward manner, which could be computationally heavy for large images. Other libraries may use optimized routines or hardware acceleration to speed up these operations.","title":"Note on Simplifications and Differences from Other Libraries:"},{"location":"pages/image-preprocessing-and-enhancement/#image-transformation-methods-in-visualalgoimagepreprocessingandenhancementtransform","text":"The Transform class implements various image transformation operations, each using an associated 3x3 transformation matrix. These operations include translation, scaling, rotation, and shear. Translation : static Matrix translate(const Matrix &image, int dx, int dy, InterpolationType method = InterpolationType::NEAREST) Moves an image by dx units in the x-direction and dy units in the y-direction. The translation matrix is: \\begin{pmatrix} 1 & 0 & dx \\\\ 0 & 1 & dy \\\\ 0 & 0 & 1 \\end{pmatrix} Scaling : static Matrix scale(const Matrix &image, float sx, float sy, InterpolationType method = InterpolationType::NEAREST) Resizes an image by a factor of sx in the x-direction and sy in the y-direction. The scaling matrix is: \\begin{pmatrix} sx & 0 & 0 \\\\ 0 & sy & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} Rotation : static Matrix rotate(const Matrix &image, float angle, InterpolationType method = InterpolationType::NEAREST) Rotates an image by a specified angle (in radians). The rotation matrix is: \\begin{pmatrix} cos(\\theta) & -sin(\\theta) & 0 \\\\ sin(\\theta) & cos(\\theta) & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} Shear : static Matrix shear(const Matrix &image, float kx, float ky, InterpolationType method = InterpolationType::NEAREST) Applies a shear transformation with the specified shear factors kx and ky in the x and y directions respectively. The shear matrix is: \\begin{pmatrix} 1 & kx & 0 \\\\ ky & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} The shearing factors kx and ky cannot both be equal to 1. This condition would result in a singular transformation matrix, and the image would collapse into a line. Affine : static Matrix affine(const Matrix &image, const Matrix &transform_matrix, InterpolationType method = InterpolationType::NEAREST) The above transformations call this method to apply the appropriate transformations. The affine transformation matrix is: \\begin{pmatrix} a & b & tx \\\\ c & d & ty \\\\ 0 & 0 & 1 \\end{pmatrix} Where a , b , c , and d represent scaling, rotation, and shear transformations, and tx and ty represent translations in the x and y directions, respectively. Perspective : static Matrix perspective(const Matrix &image, const Matrix &transform_matrix, InterpolationType method = InterpolationType::NEAREST) Perspective transformation is more general than the affine transformation due to the presence of non-zero values in the third column of the transformation matrix for perspective transformations. These values (denoted as h and i ) result in a transformation equivalent to shearing along the x and y-axes. \\begin{pmatrix} a & b & tx \\\\ c & d & ty \\\\ h & i & 1 \\end{pmatrix}","title":"Image Transformation Methods in VisualAlgo::ImagePreprocessingAndEnhancement::Transform"},{"location":"pages/image-preprocessing-and-enhancement/#example-usage_1","text":"In this example, we use the Transform class to apply various transformations on an image, such as translation, scaling, rotation, shearing, and perspective transformation. #include \"TestHarness.h\" #include \"ImagePreprocessingAndEnhancement/Transform.hpp\" #include \"helpers/Matrix.hpp\" #include <string> #include <cmath> VisualAlgo::Matrix image; image.load(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_resized.ppm\"); // Translate the image VisualAlgo::Matrix image_translated = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::translate(image, 100, 50); image_translated.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_translated.ppm\", true); // Scale the image VisualAlgo::Matrix image_scaled = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::scale(image, 2, 0.5, VisualAlgo::ImagePreprocessingAndEnhancement::InterpolationType::NEAREST); image_scaled.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_scaled.ppm\", true); // Rotate the image VisualAlgo::Matrix image_rotated = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::rotate(image, M_PI / 2); image_rotated.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_rotated.ppm\", true); // Shear the image VisualAlgo::Matrix image_sheared = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::shear(image, 0, 0.5); image_sheared.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_sheared.ppm\", true); // Apply perspective transformation on the image VisualAlgo::Matrix perspective_matrix = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::scale(0.5, 0.5); perspective_matrix.set(2, 0, 0.01); perspective_matrix.set(2, 1, 0.01); VisualAlgo::Matrix image_perspective_transformed = VisualAlgo::ImagePreprocessingAndEnhancement::Transform::perspective(image, perspective_matrix); image_perspective_transformed.save(\"datasets/ImagePreprocessingAndEnhancement/lighthouse_perspective.ppm\", true);","title":"Example Usage"},{"location":"pages/image-preprocessing-and-enhancement/#visual-examples_1","text":"Here are the results from the above code: Translation Scaling Rotation Shearing Perspective","title":"Visual Examples"},{"location":"pages/image-preprocessing-and-enhancement/#histogram-equalization","text":"","title":"Histogram Equalization"},{"location":"pages/image-preprocessing-and-enhancement/#neuroscience-models","text":"","title":"Neuroscience Models"},{"location":"pages/learning-and-memory/","text":"9. Learning and Memory Computer Vision Algorithms Deep Learning Models (CNN, Autoencoders, GAN, Transformers) Neuroscience Models Hopfield Network Adaptive Resonance Theory (ART)","title":"9. Learning and Memory"},{"location":"pages/learning-and-memory/#9-learning-and-memory","text":"","title":"9. Learning and Memory"},{"location":"pages/learning-and-memory/#computer-vision-algorithms","text":"Deep Learning Models (CNN, Autoencoders, GAN, Transformers)","title":"Computer Vision Algorithms"},{"location":"pages/learning-and-memory/#neuroscience-models","text":"Hopfield Network Adaptive Resonance Theory (ART)","title":"Neuroscience Models"},{"location":"pages/matrix/","text":"VisualAlgo::Matrix Struct Documentation The Matrix struct provides a two-dimensional matrix object, along with numerous methods for performing operations on the matrix. Include #include \"helpers/Matrix.hpp\" Struct Attributes rows ( int ): The number of rows in the matrix. cols ( int ): The number of columns in the matrix. data ( vector<vector<float>> ): A 2D vector that holds the matrix data. Constructors Matrix(int rows, int cols) : Constructs a new Matrix object with the given number of rows and columns. The matrix is initialized with all elements set to 0. VisualAlgo::Matrix m(3, 4); // Creates a 3x4 matrix with all elements initialized to 0 Matrix(int rows, int cols, float value) : Constructs a new Matrix object with the given number of rows and columns. The matrix is initialized with all elements set to the provided value. VisualAlgo::Matrix m(3, 4, 1.0); // Creates a 3x4 matrix with all elements initialized to 1.0 Matrix(vector<vector<float>> data) : Constructs a new Matrix object with the provided 2D vector data. std::vector<std::vector<float>> data = {{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}; VisualAlgo::Matrix m(data); // Creates a 2x3 matrix with the provided data or VisualAlgo::Matrix m({{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}); Matrix(const Matrix &other) : Copy constructor. Constructs a new Matrix object that is a copy of the provided matrix. VisualAlgo::Matrix m1(3, 4, 1.0); VisualAlgo::Matrix m2(m1); // Creates a new matrix that is a copy of m1 Special Matrix Constructors Matrix::zeros(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to 0. VisualAlgo::Matrix m = VisualAlgo::Matrix::zeros(3, 4); // Creates a 3x4 matrix with all elements initialized to 0 Matrix::ones(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to 1. VisualAlgo::Matrix m = VisualAlgo::Matrix::ones(3, 4); // Creates a 3x4 matrix with all elements initialized to 1 Matrix::eye(int rows, int cols) : Returns a new identity Matrix object with the specified number of rows and columns. All diagonal elements are set to 1, and all other elements are set to 0. VisualAlgo::Matrix m = VisualAlgo::Matrix::eye(3, 3); // Creates a 3x3 identity matrix Random Matrix Constructors Matrix::random(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to a random floating point value in the range [0, 1]. VisualAlgo::Matrix m = VisualAlgo::Matrix::random(3, 4); // Creates a 3x4 matrix with all elements initialized to a random value Matrix::random(int rows, int cols, float min, float max) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to a random floating point value in the range [min, max]. VisualAlgo::Matrix m = VisualAlgo::Matrix::random(3, 4, -1.0, 1.0); // Creates a 3x4 matrix with all elements initialized to a random value between -1.0 and 1.0 Element-wise Operations Element-wise operations perform the operation on each element of the matrix independently. Matrix &operator=(const Matrix &other) : Assignment operator. Copies the provided matrix to the current matrix. VisualAlgo::Matrix m1(3, 4, 1.0); VisualAlgo::Matrix m2 = m1; // m2 is now a copy of m1 Matrix operator+(const Matrix &other) : Adds the provided matrix to the current matrix. VisualAlgo::Matrix m1(2, 2, 1.0); VisualAlgo::Matrix m2(2, 2, 2.0); VisualAlgo::Matrix m3 = m1 + m2; // m3 is now a 2x2 matrix with all elements set to 3.0 Matrix operator+(const float &other) : Add the provided float to all entries. Also support other common arithmetic operators. Element-wise Comparison Functions Matrix::elementwise_max(const Matrix &a, const Matrix &b) : Returns a new Matrix that is the element-wise maximum of matrices a and b . The dimensions of a and b must be the same. VisualAlgo::Matrix a = VisualAlgo::Matrix::ones(3, 4); VisualAlgo::Matrix b = VisualAlgo::Matrix::zeros(3, 4); VisualAlgo::Matrix m = VisualAlgo::Matrix::elementwise_max(a, b); // m will be a 3x4 matrix with all elements equal to 1 Matrix::elementwise_min(const Matrix &a, const Matrix &b) : Returns a new Matrix that is the element-wise minimum of matrices a and b . The dimensions of a and b must be the same. VisualAlgo::Matrix a = VisualAlgo::Matrix::ones(3, 4); VisualAlgo::Matrix b = VisualAlgo::Matrix::zeros(3, 4); VisualAlgo::Matrix m = VisualAlgo::Matrix::elementwise_min(a, b); // m will be a 3x4 matrix with all elements equal to 0 The Matrix struct supports a variety of comparison operations for comparing two Matrix objects or a Matrix object with a float. bool operator==(const Matrix &other) const : Compares the current matrix with other for equality. Returns true if all elements in the two matrices are exactly equal, and false otherwise. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{1, 2, 3}, {4, 5, 6}}); bool isEqual = (m1 == m2); // Returns true bool operator!=(const Matrix &other) const : Compares the current matrix with other for inequality. Returns true if any element in the two matrices is not equal, and false otherwise. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{1, 2, 3}, {7, 8, 9}}); bool isNotEqual = (m1 != m2); // Returns true bool is_close(const Matrix &other, float tolerance=1e-5) const : Compares the current matrix with other within a given tolerance . Returns true if the absolute difference between each corresponding pair of elements in the two matrices is less than or equal to the tolerance , and false otherwise. Matrix m1({{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}); Matrix m2({{1.00001, 2.00001, 3.00001}, {3.99999, 5.00001, 5.99999}}); bool isClose = m1.is_close(m2); // Returns true Matrix operator>(const Matrix &other) const : Returns a binary matrix with 1 s where the corresponding element in the current matrix is greater than that in other , and 0 s elsewhere. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{-1, 2, 4}, {-99, 55, 0}}); Matrix m3 = m1 > m2; // Returns a binary matrix Matrix operator<(const Matrix &other) const : Similar to the > operator, but checks for less than. Matrix operator>=(const Matrix &other) const : Similar to the > operator, but checks for greater than or equal to. Matrix operator<=(const Matrix &other) const : Similar to the > operator, but checks for less than or equal to. In addition to the matrix-matrix comparison operators, there are similar matrix-float comparison operators for comparing each element in a matrix to a float. These are operator>(const float &other) const , operator<(const float &other) const , operator>=(const float &other) const , and operator<=(const float &other) const . Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2 = m1 > 3; // Returns a binary matrix with `1`s where the elements in m1 are greater than 3 and `0`s elsewhere Matrix Operations Matrix transpose() const : Returns the transpose of the current matrix. VisualAlgo::Matrix m1(2, 3, 1.0); VisualAlgo::Matrix m2 = m1.transpose(); // m2 is now a 3x2 matrix Matrix Matrix::submatrix(int row_start, int row_end, int col_start, int col_end) const : Returns a submatrix of the current matrix from row_start to row_end (exclusive) and col_start to col_end (exclusive). VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.submatrix(1, 3, 0, 2); // m2 is now a 2x2 matrix float Matrix::det() const : Calculates the determinant of the current matrix. Only applicable for square matrices. VisualAlgo::Matrix m1(3, 3, 1.0); float result = m1.det(); // result is the determinant of m1 Matrix Matrix::cofactor() const : Returns the cofactor matrix of the current matrix. Only applicable for square matrices. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.cofactor(); // m2 is the cofactor matrix of m1 Matrix Matrix::inverse() const : Inverts the current matrix. Only applicable for square matrices that are invertible. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.inverse(); float dot(const Matrix &other) const : Calculates the dot product of the current matrix with the provided matrix. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2(3, 3, 2.0); float result = m1.dot(m2); // result is now 18 Matrix Matrix::matmul(const Matrix &other) const : Matrix multiplication. auto m1 = Matrix({{1, 2, 3}, {4, 5, 6}}); auto m2 = Matrix({{10, 11}, {20, 21}, {30, 31}}); auto m3 = m1.matmul(m2); Accessors void set(int row, int col, float value) : Sets the value at the specified row and column in the matrix. VisualAlgo::Matrix m(3, 4, 0.0); m.set(1, 2, 1.0); // Sets the value at row 1, column 2 to 1.0 const float get(int row, int col) const : Returns the value at the specified row and column in the matrix. VisualAlgo::Matrix m(3, 4, 1.0); float value = m.get(2, 3); // Gets the value at row 2, column 3 std::vector<float> &operator[](int row) : Returns the row at the specified index in the matrix. VisualAlgo::Matrix m(3, 4, 1.0); std::vector<float> row = m[1]; // Gets the second row of the matrix Statistics float sum() : Returns the sum of all elements in the matrix. VisualAlgo::Matrix m(2, 3, 1.0); float sum = m.sum(); // sum is now 6.0 float mean() : Returns the mean (average) of all elements in the matrix. VisualAlgo::Matrix m(2, 3, 1.0); float mean = m.mean(); // mean is now 1.0 Also supports std() , max() , and min() operations. Image Operations void Matrix::load(const std::string &filename) : This function loads an image from the specified file path and stores it as a grayscale matrix. The function can only handle images in PPM format (P6). If the file doesn't exist or the file format is not P6, it will throw a runtime error. VisualAlgo::Matrix m; m.load(\"path_to_your_image.ppm\"); // Loads the image from the specified path This function reads the image file as a binary file. It first reads the header to ensure that the image is in the correct format (P6), then reads the width and height of the image. The pixel data is then read and converted from RGB to grayscale using the ITU-R BT.709 luma transform. The grayscale pixel values are stored in the data member of the Matrix object. Matrix Matrix::load(const std::string &filename, int rows, int cols) : Directly load an image. VisualAlgo::Matrix m = VisualAlgo::Matrix.load(\"path_to_your_image.ppm\", 256, 256); void Matrix::save(const std::string &filename, bool normalize) const : This function saves the matrix as an image to the specified file path. The image is saved in PPM format (P6). The normalize parameter specifies whether the matrix data should be normalized to the range 0-255 before saving. If not normalized and the pixel values are not within this range, a runtime error is thrown. VisualAlgo::Matrix m(3, 4, 1.0); m.save(\"path_to_save_image.ppm\", true); // Normalizes and saves the matrix as an image to the specified path This function first checks if the data needs normalization based on the normalize flag. If true, it normalizes the data in the Matrix object to the range 0-255 using the normalize255() function. Then it writes the image data to the file in PPM format (P6). The pixel data is written as RGB, where the R, G, and B values are all equal, resulting in a grayscale image. void Matrix::normalize() : This function normalizes the values in the matrix to the range [0, 1]. VisualAlgo::Matrix m(3, 4, 1.0); m.normalize(); // Normalizes the matrix values to the range [0, 1.0] void Matrix::normalize255() : This function normalizes the values in the matrix to the range [0.0, 255.0]. This is useful to prepare the data for saving as an image, since pixel values in an image must be in this range. VisualAlgo::Matrix m(3, 4, 1.0); m.normalize255(); // Normalizes the matrix values to the range [0, 255.0] void Matrix::relu() : This function applies the ReLU (Rectified Linear Unit) operation to the matrix. It replaces all negative pixel values with zeros, effectively achieving half-wave rectification. VisualAlgo::Matrix m(3, 4, -1.0); m.relu(); // Changes all negative values to 0 void abs() : This function take the absolute value of all the entries. Matrix Matrix::cross_correlate(const VisualAlgo::Matrix &kernel, int padding, int stride) const : This function performs the cross-correlation operation between the matrix and the provided kernel. The padding and stride parameters control the operation. If the kernel size is larger than the matrix or the stride is less than or equal to zero, or padding is negative, it will throw an invalid_argument exception. VisualAlgo::Matrix m(3, 4, 1.0); VisualAlgo::Matrix kernel(2, 2, 0.5); VisualAlgo::Matrix result = m.cross_correlate(kernel, 1, 2); // Perform cross-correlation This function creates an output matrix of appropriate size based on the input matrix, kernel, padding, and stride. It then performs the cross-correlation operation and stores the result in the output matrix. Matrix Matrix::cross_correlate(const VisualAlgo::Matrix &kernel) const : This function performs cross-correlation on the matrix with the provided kernel. The output matrix is always the same size as the input matrix. This operation effectively considers there to be zero padding beyond the edges of the original matrix and will wrap around when indexing beyond its dimensions. Matrix Matrix::cross_correlate_full(const VisualAlgo::Matrix &kernel) const : This function performs cross-correlation operation on the matrix with the provided kernel, with zero padding. The output matrix size is larger than the input matrix size, taking into account the kernel size and the full overlap. Matrix Matrix::flip() const : This function returns a new matrix which is the flipped version of the current matrix. This is particularly useful when trying to perform convolution using a kernel, as convolution is mathematically the same as cross-correlation with a flipped kernel. Matrix Matrix::convolve(const VisualAlgo::Matrix &kernel, int padding, int stride) const : This function performs convolution between the matrix and the provided kernel. It first flips the kernel and then performs cross-correlation. The padding and stride parameters are similar to the cross-correlation function. Matrix Matrix::convolve(const VisualAlgo::Matrix &kernel) const : This function performs convolution on the matrix with the provided kernel and keeps the same size. This operation effectively considers there to be zero padding beyond the edges of the original matrix and will wrap around when indexing beyond its dimensions.","title":"Matrix"},{"location":"pages/matrix/#visualalgomatrix-struct-documentation","text":"The Matrix struct provides a two-dimensional matrix object, along with numerous methods for performing operations on the matrix.","title":"VisualAlgo::Matrix Struct Documentation"},{"location":"pages/matrix/#include","text":"#include \"helpers/Matrix.hpp\"","title":"Include"},{"location":"pages/matrix/#struct-attributes","text":"rows ( int ): The number of rows in the matrix. cols ( int ): The number of columns in the matrix. data ( vector<vector<float>> ): A 2D vector that holds the matrix data.","title":"Struct Attributes"},{"location":"pages/matrix/#constructors","text":"Matrix(int rows, int cols) : Constructs a new Matrix object with the given number of rows and columns. The matrix is initialized with all elements set to 0. VisualAlgo::Matrix m(3, 4); // Creates a 3x4 matrix with all elements initialized to 0 Matrix(int rows, int cols, float value) : Constructs a new Matrix object with the given number of rows and columns. The matrix is initialized with all elements set to the provided value. VisualAlgo::Matrix m(3, 4, 1.0); // Creates a 3x4 matrix with all elements initialized to 1.0 Matrix(vector<vector<float>> data) : Constructs a new Matrix object with the provided 2D vector data. std::vector<std::vector<float>> data = {{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}; VisualAlgo::Matrix m(data); // Creates a 2x3 matrix with the provided data or VisualAlgo::Matrix m({{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}); Matrix(const Matrix &other) : Copy constructor. Constructs a new Matrix object that is a copy of the provided matrix. VisualAlgo::Matrix m1(3, 4, 1.0); VisualAlgo::Matrix m2(m1); // Creates a new matrix that is a copy of m1","title":"Constructors"},{"location":"pages/matrix/#special-matrix-constructors","text":"Matrix::zeros(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to 0. VisualAlgo::Matrix m = VisualAlgo::Matrix::zeros(3, 4); // Creates a 3x4 matrix with all elements initialized to 0 Matrix::ones(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to 1. VisualAlgo::Matrix m = VisualAlgo::Matrix::ones(3, 4); // Creates a 3x4 matrix with all elements initialized to 1 Matrix::eye(int rows, int cols) : Returns a new identity Matrix object with the specified number of rows and columns. All diagonal elements are set to 1, and all other elements are set to 0. VisualAlgo::Matrix m = VisualAlgo::Matrix::eye(3, 3); // Creates a 3x3 identity matrix","title":"Special Matrix Constructors"},{"location":"pages/matrix/#random-matrix-constructors","text":"Matrix::random(int rows, int cols) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to a random floating point value in the range [0, 1]. VisualAlgo::Matrix m = VisualAlgo::Matrix::random(3, 4); // Creates a 3x4 matrix with all elements initialized to a random value Matrix::random(int rows, int cols, float min, float max) : Returns a new Matrix object with the specified number of rows and columns. All elements of the matrix are initialized to a random floating point value in the range [min, max]. VisualAlgo::Matrix m = VisualAlgo::Matrix::random(3, 4, -1.0, 1.0); // Creates a 3x4 matrix with all elements initialized to a random value between -1.0 and 1.0","title":"Random Matrix Constructors"},{"location":"pages/matrix/#element-wise-operations","text":"Element-wise operations perform the operation on each element of the matrix independently. Matrix &operator=(const Matrix &other) : Assignment operator. Copies the provided matrix to the current matrix. VisualAlgo::Matrix m1(3, 4, 1.0); VisualAlgo::Matrix m2 = m1; // m2 is now a copy of m1 Matrix operator+(const Matrix &other) : Adds the provided matrix to the current matrix. VisualAlgo::Matrix m1(2, 2, 1.0); VisualAlgo::Matrix m2(2, 2, 2.0); VisualAlgo::Matrix m3 = m1 + m2; // m3 is now a 2x2 matrix with all elements set to 3.0 Matrix operator+(const float &other) : Add the provided float to all entries. Also support other common arithmetic operators.","title":"Element-wise Operations"},{"location":"pages/matrix/#element-wise-comparison-functions","text":"Matrix::elementwise_max(const Matrix &a, const Matrix &b) : Returns a new Matrix that is the element-wise maximum of matrices a and b . The dimensions of a and b must be the same. VisualAlgo::Matrix a = VisualAlgo::Matrix::ones(3, 4); VisualAlgo::Matrix b = VisualAlgo::Matrix::zeros(3, 4); VisualAlgo::Matrix m = VisualAlgo::Matrix::elementwise_max(a, b); // m will be a 3x4 matrix with all elements equal to 1 Matrix::elementwise_min(const Matrix &a, const Matrix &b) : Returns a new Matrix that is the element-wise minimum of matrices a and b . The dimensions of a and b must be the same. VisualAlgo::Matrix a = VisualAlgo::Matrix::ones(3, 4); VisualAlgo::Matrix b = VisualAlgo::Matrix::zeros(3, 4); VisualAlgo::Matrix m = VisualAlgo::Matrix::elementwise_min(a, b); // m will be a 3x4 matrix with all elements equal to 0 The Matrix struct supports a variety of comparison operations for comparing two Matrix objects or a Matrix object with a float. bool operator==(const Matrix &other) const : Compares the current matrix with other for equality. Returns true if all elements in the two matrices are exactly equal, and false otherwise. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{1, 2, 3}, {4, 5, 6}}); bool isEqual = (m1 == m2); // Returns true bool operator!=(const Matrix &other) const : Compares the current matrix with other for inequality. Returns true if any element in the two matrices is not equal, and false otherwise. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{1, 2, 3}, {7, 8, 9}}); bool isNotEqual = (m1 != m2); // Returns true bool is_close(const Matrix &other, float tolerance=1e-5) const : Compares the current matrix with other within a given tolerance . Returns true if the absolute difference between each corresponding pair of elements in the two matrices is less than or equal to the tolerance , and false otherwise. Matrix m1({{1.0, 2.0, 3.0}, {4.0, 5.0, 6.0}}); Matrix m2({{1.00001, 2.00001, 3.00001}, {3.99999, 5.00001, 5.99999}}); bool isClose = m1.is_close(m2); // Returns true Matrix operator>(const Matrix &other) const : Returns a binary matrix with 1 s where the corresponding element in the current matrix is greater than that in other , and 0 s elsewhere. Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2({{-1, 2, 4}, {-99, 55, 0}}); Matrix m3 = m1 > m2; // Returns a binary matrix Matrix operator<(const Matrix &other) const : Similar to the > operator, but checks for less than. Matrix operator>=(const Matrix &other) const : Similar to the > operator, but checks for greater than or equal to. Matrix operator<=(const Matrix &other) const : Similar to the > operator, but checks for less than or equal to. In addition to the matrix-matrix comparison operators, there are similar matrix-float comparison operators for comparing each element in a matrix to a float. These are operator>(const float &other) const , operator<(const float &other) const , operator>=(const float &other) const , and operator<=(const float &other) const . Matrix m1({{1, 2, 3}, {4, 5, 6}}); Matrix m2 = m1 > 3; // Returns a binary matrix with `1`s where the elements in m1 are greater than 3 and `0`s elsewhere","title":"Element-wise Comparison Functions"},{"location":"pages/matrix/#matrix-operations","text":"Matrix transpose() const : Returns the transpose of the current matrix. VisualAlgo::Matrix m1(2, 3, 1.0); VisualAlgo::Matrix m2 = m1.transpose(); // m2 is now a 3x2 matrix Matrix Matrix::submatrix(int row_start, int row_end, int col_start, int col_end) const : Returns a submatrix of the current matrix from row_start to row_end (exclusive) and col_start to col_end (exclusive). VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.submatrix(1, 3, 0, 2); // m2 is now a 2x2 matrix float Matrix::det() const : Calculates the determinant of the current matrix. Only applicable for square matrices. VisualAlgo::Matrix m1(3, 3, 1.0); float result = m1.det(); // result is the determinant of m1 Matrix Matrix::cofactor() const : Returns the cofactor matrix of the current matrix. Only applicable for square matrices. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.cofactor(); // m2 is the cofactor matrix of m1 Matrix Matrix::inverse() const : Inverts the current matrix. Only applicable for square matrices that are invertible. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2 = m1.inverse(); float dot(const Matrix &other) const : Calculates the dot product of the current matrix with the provided matrix. VisualAlgo::Matrix m1(3, 3, 1.0); VisualAlgo::Matrix m2(3, 3, 2.0); float result = m1.dot(m2); // result is now 18 Matrix Matrix::matmul(const Matrix &other) const : Matrix multiplication. auto m1 = Matrix({{1, 2, 3}, {4, 5, 6}}); auto m2 = Matrix({{10, 11}, {20, 21}, {30, 31}}); auto m3 = m1.matmul(m2);","title":"Matrix Operations"},{"location":"pages/matrix/#accessors","text":"void set(int row, int col, float value) : Sets the value at the specified row and column in the matrix. VisualAlgo::Matrix m(3, 4, 0.0); m.set(1, 2, 1.0); // Sets the value at row 1, column 2 to 1.0 const float get(int row, int col) const : Returns the value at the specified row and column in the matrix. VisualAlgo::Matrix m(3, 4, 1.0); float value = m.get(2, 3); // Gets the value at row 2, column 3 std::vector<float> &operator[](int row) : Returns the row at the specified index in the matrix. VisualAlgo::Matrix m(3, 4, 1.0); std::vector<float> row = m[1]; // Gets the second row of the matrix","title":"Accessors"},{"location":"pages/matrix/#statistics","text":"float sum() : Returns the sum of all elements in the matrix. VisualAlgo::Matrix m(2, 3, 1.0); float sum = m.sum(); // sum is now 6.0 float mean() : Returns the mean (average) of all elements in the matrix. VisualAlgo::Matrix m(2, 3, 1.0); float mean = m.mean(); // mean is now 1.0 Also supports std() , max() , and min() operations.","title":"Statistics"},{"location":"pages/matrix/#image-operations","text":"void Matrix::load(const std::string &filename) : This function loads an image from the specified file path and stores it as a grayscale matrix. The function can only handle images in PPM format (P6). If the file doesn't exist or the file format is not P6, it will throw a runtime error. VisualAlgo::Matrix m; m.load(\"path_to_your_image.ppm\"); // Loads the image from the specified path This function reads the image file as a binary file. It first reads the header to ensure that the image is in the correct format (P6), then reads the width and height of the image. The pixel data is then read and converted from RGB to grayscale using the ITU-R BT.709 luma transform. The grayscale pixel values are stored in the data member of the Matrix object. Matrix Matrix::load(const std::string &filename, int rows, int cols) : Directly load an image. VisualAlgo::Matrix m = VisualAlgo::Matrix.load(\"path_to_your_image.ppm\", 256, 256); void Matrix::save(const std::string &filename, bool normalize) const : This function saves the matrix as an image to the specified file path. The image is saved in PPM format (P6). The normalize parameter specifies whether the matrix data should be normalized to the range 0-255 before saving. If not normalized and the pixel values are not within this range, a runtime error is thrown. VisualAlgo::Matrix m(3, 4, 1.0); m.save(\"path_to_save_image.ppm\", true); // Normalizes and saves the matrix as an image to the specified path This function first checks if the data needs normalization based on the normalize flag. If true, it normalizes the data in the Matrix object to the range 0-255 using the normalize255() function. Then it writes the image data to the file in PPM format (P6). The pixel data is written as RGB, where the R, G, and B values are all equal, resulting in a grayscale image. void Matrix::normalize() : This function normalizes the values in the matrix to the range [0, 1]. VisualAlgo::Matrix m(3, 4, 1.0); m.normalize(); // Normalizes the matrix values to the range [0, 1.0] void Matrix::normalize255() : This function normalizes the values in the matrix to the range [0.0, 255.0]. This is useful to prepare the data for saving as an image, since pixel values in an image must be in this range. VisualAlgo::Matrix m(3, 4, 1.0); m.normalize255(); // Normalizes the matrix values to the range [0, 255.0] void Matrix::relu() : This function applies the ReLU (Rectified Linear Unit) operation to the matrix. It replaces all negative pixel values with zeros, effectively achieving half-wave rectification. VisualAlgo::Matrix m(3, 4, -1.0); m.relu(); // Changes all negative values to 0 void abs() : This function take the absolute value of all the entries. Matrix Matrix::cross_correlate(const VisualAlgo::Matrix &kernel, int padding, int stride) const : This function performs the cross-correlation operation between the matrix and the provided kernel. The padding and stride parameters control the operation. If the kernel size is larger than the matrix or the stride is less than or equal to zero, or padding is negative, it will throw an invalid_argument exception. VisualAlgo::Matrix m(3, 4, 1.0); VisualAlgo::Matrix kernel(2, 2, 0.5); VisualAlgo::Matrix result = m.cross_correlate(kernel, 1, 2); // Perform cross-correlation This function creates an output matrix of appropriate size based on the input matrix, kernel, padding, and stride. It then performs the cross-correlation operation and stores the result in the output matrix. Matrix Matrix::cross_correlate(const VisualAlgo::Matrix &kernel) const : This function performs cross-correlation on the matrix with the provided kernel. The output matrix is always the same size as the input matrix. This operation effectively considers there to be zero padding beyond the edges of the original matrix and will wrap around when indexing beyond its dimensions. Matrix Matrix::cross_correlate_full(const VisualAlgo::Matrix &kernel) const : This function performs cross-correlation operation on the matrix with the provided kernel, with zero padding. The output matrix size is larger than the input matrix size, taking into account the kernel size and the full overlap. Matrix Matrix::flip() const : This function returns a new matrix which is the flipped version of the current matrix. This is particularly useful when trying to perform convolution using a kernel, as convolution is mathematically the same as cross-correlation with a flipped kernel. Matrix Matrix::convolve(const VisualAlgo::Matrix &kernel, int padding, int stride) const : This function performs convolution between the matrix and the provided kernel. It first flips the kernel and then performs cross-correlation. The padding and stride parameters are similar to the cross-correlation function. Matrix Matrix::convolve(const VisualAlgo::Matrix &kernel) const : This function performs convolution on the matrix with the provided kernel and keeps the same size. This operation effectively considers there to be zero padding beyond the edges of the original matrix and will wrap around when indexing beyond its dimensions.","title":"Image Operations"},{"location":"pages/motion-analysis-and-tracking/","text":"5. Motion Analysis and Tracking Computer Vision Algorithms Optical Flow Background Subtraction CAMShift Neuroscience Models Reichardt Detectors Motion Energy Models","title":"5. Motion Analysis and Tracking"},{"location":"pages/motion-analysis-and-tracking/#5-motion-analysis-and-tracking","text":"","title":"5. Motion Analysis and Tracking"},{"location":"pages/motion-analysis-and-tracking/#computer-vision-algorithms","text":"Optical Flow Background Subtraction CAMShift","title":"Computer Vision Algorithms"},{"location":"pages/motion-analysis-and-tracking/#neuroscience-models","text":"Reichardt Detectors Motion Energy Models","title":"Neuroscience Models"},{"location":"pages/object-detection-and-recognition/","text":"4. Object Detection and Recognition Computer Vision Algorithms Viola-Jones Haar Cascades R-CNN SSD YOLO Neuroscience Models HMAX Model","title":"4. Object Detection and Recognition"},{"location":"pages/object-detection-and-recognition/#4-object-detection-and-recognition","text":"","title":"4. Object Detection and Recognition"},{"location":"pages/object-detection-and-recognition/#computer-vision-algorithms","text":"Viola-Jones Haar Cascades R-CNN SSD YOLO","title":"Computer Vision Algorithms"},{"location":"pages/object-detection-and-recognition/#neuroscience-models","text":"HMAX Model","title":"Neuroscience Models"},{"location":"pages/segmentation-and-grouping/","text":"3. Segmentation and Grouping Computer Vision Algorithms Region Growing Watershed K-means Clustering Mean Shift GrabCut U-Net Classical Region Proposal Methods Neuroscience Models The FBF Model Reference : Grossberg and Wyse, \"Figure-Ground Separation of Connected Scenic Figures: Boundaries, Filling-In, and Opponent Processing\", Neural Networks For Vision and Image Processing , Chapter 7, 1992. (Link to paper with the same topic but low scan quality) Overview : The FBF network model was used as part of the larger pattern recognition model by Grossberg et al.. It handles the initial steps of processing, up until the figure-ground separation phase. The FBF model is composed of two sub-systems: the Feature Contour System (FCS) and the Boundary Contour System (BCS), both initially proposed by Grossberg and Mingolla in 1985. As the processing sequence typically follows the pattern FCS-BCS-FCS, these combined systems are collectively referred to as the FBF networks. The central idea posited by Grossberg is that these two systems collaboratively enable us to recognize surfaces (handled by the FCS) and boundaries (handled by the BCS). The FCS's role is to fill in the entire boundary region with consistent surface characteristics, relying on the boundaries computed by the BCS, which connects collinear contours to establish these boundaries. A notable point raised by Grossberg is our ability to recognize boundaries even when they're not directly visible, leading to illusions such as the Kanizsa triangle. Please note that the FCS and BCS are hypothetical models for the visual system, distinguished by their function (\"what\" they are supposed to do) rather than their mechanism (\"how\" they are supposed to operate). These models have evolved significantly over time, and their definition is not bound by any specific method or implementation. The exact details often depend on specific research papers. Also, note that I've made a number of simplifications to the original model. For example, the original paper applied mathematics based on sub-pixel resolution (interpolating pixels), whereas the simple cells I used are entirely pixel-based. Algorithm Breakdown : The FBF model algorithm consists of four steps: Step 1: Discount the Illuminant : Instead of uniformly applying the ON-Center-OFF-Surround (ON-C) and OFF-Center-ON-Surround (OFF-C) kernels (also known as Difference of Gaussian), the kernel is scaled with the image value to handle areas with varying brightness levels. Grossberg describes these cells as \"shunting\" because their dynamics are dependent on their current state, akin to an electrical system with a (shunting) resistor. This is believed to be part of the FCS's work, as it needs to perceive surfaces as continuous over a broad range of illumination levels. The output of the ON-C and OFF-C processes are denoted as \\mathbf{x} and \\bar{\\mathbf{x}} respectively: \\mathbf{x} = \\frac{\\mathbf{I} \\otimes (B \\mathbf{C} - D \\mathbf{E})}{A + \\mathbf{I} \\otimes (\\mathbf{C} + \\mathbf{E})}\\\\ \\bar{\\mathbf{x}} = \\frac{A \\cdot S + \\mathbf{I} \\otimes (D \\mathbf{E} - B \\mathbf{C})}{A + \\mathbf{I} \\otimes (\\mathbf{C} + \\mathbf{E})} Here, the symbol \\otimes represents the operation of 2D cross-correlation. \\mathbf{I} represents the input image array, while A , B , and D are constants that define the shape of the ON-C and OFF-C kernels. The term S introduces the offset of the OFF-C kernel, ensuring that \\bar{\\mathbf{x}} primarily falls within the positive range. The values for those parameters can also be found in the paper. \\mathbf{C} and \\mathbf{E} denote two Gaussian kernels, as the ON-C and OFF-C kernels are \"Differences of Gaussians\" (DoG). The ON-C shape is derived from subtracting the wider and shorter Gaussian kernel \\mathbf{E} from the narrower and taller Gaussian kernel \\mathbf{C} . For the OFF-C kernel, the procedure is reversed. The formulas for these kernels are: \\mathbf{C} = C e^{-\\frac{(p - i)^2 + (q - j)^2}{\\alpha^2}}\\\\ \\mathbf{E} = E e^{-\\frac{(p - i)^2 + (q - j)^2}{\\beta^2}}\\\\ where \\alpha and \\beta are two other constants. (Interestingly, Grossberg and Wyse applied a logarithmic transformation to the arguments of the exponent in their paper, a step that I did not use in my implementation. Instead, I use the \"standard\" form of Gaussian shown above.) Step 2: CORT-X 2 Filter : The CORT-X 2 Filter is a neurophysiologically inspired model that draws on Hubel and Wiesel's work on Simple, Complex, and Hypercomplex cells. It forms the BCS part of this model. The filter takes the output from the previous step as its input, and generates the boundaries. It operates in a purely feedforward manner and can be broken down into six sub-steps: Step 2a: Simple Cells : These cells receive either \\mathbf{x} or \\bar{\\mathbf{x}} as inputs. The are two scales kernels s for the kernels: s=1 , with major and minor axes of 12 and 6 pixels, and s=2 , with major and minor axes of 20 and 10 pixels. The kernel, \\mathbf{K} , comprises two ellipse halves, \\mathbf{L_s} and \\mathbf{R_s} : \\mathbf{K}_{s, L} (k) = \\mathbf{L_s} (k) - \\alpha_s \\mathbf{R_s} (k) - \\beta_s \\mathbf{K}_{s, R} (k) = \\mathbf{R_s} (k) - \\alpha_s \\mathbf{L_s} (k) - \\beta_s Here, k denotes the orientation index of the kernel, ranging from k_0 = 0, k_1 = 22.5, ... to k_7 = 157.5 degrees. For clarity, note that Grossberg and Wyse didn't just rotate the simple cell kernel by 360 degrees. Instead, they rotate two kernels, \\mathbf{L_s} and \\mathbf{R_s} , with opposite polarities by 180 degrees. This is because the kernels of the same orientation and opposite polarity will combine in the next step, making this notation more intuitive. The output of the simple cells comes from the cross-correlation between the ON-C (or OFF-C) signals, followed by half-wave rectification: \\mathbf{S^+_{s, L}} (k) = \\max(\\mathbf{K}_{s, L} (k) \\otimes \\mathbf{x}, 0) \\mathbf{S^+_{s, R}} (k) = \\max(\\mathbf{K}_{s, R} (k) \\otimes \\mathbf{x}, 0) \\mathbf{S^-_{s, L}} (k) = \\max(\\mathbf{K}_{s, L} (k) \\otimes \\bar{\\mathbf{x}}, 0) \\mathbf{S^-_{s, R}} (k) = \\max(\\mathbf{K}_{s, R} (k) \\otimes \\bar{\\mathbf{x}}, 0) Step 2b: Complex Cells : The complex cells combine the responses from simple cells of the same orientation. Be aware that the \\mathbf{C} notation here represents the response map of complex cells, not the Gaussian from Step 1. \\mathbf{C}_s (k) = F \\left[ \\mathbf{S^+_{s, L}} (k) + \\mathbf{S^+_{s, R}} (k) + \\mathbf{S^-_{s, L}} (k) + \\mathbf{S^-_{s, R}} (k) \\right] Here, F is another constant parameter. Half-wave rectification isn't necessary as the output is always non-negative. Step 2c: Hypercomplex Cells (First Competitive Stage) : This step resembles the non-max suppression in Canny edge detection as it aims to inhibit texture around boundaries that lack other collinear edges. This is accomplished by dividing the complex cells response map by the cross-correlation between the complex cell response and the oriented competition kernel \\mathbf{G_s} (k) . This kernel is normalized and positive everywhere except along the direction of the complex cell's corresponding orientation. Consequently, if there are non-collinear edges in the vicinity, the cross-correlation term in the denominator becomes large. \\mathbf{D}_{s} (k) = \\max \\left[\\frac{\\mathbf{C_s}(k)}{\\epsilon + \\mu \\sum_m (\\mathbf{C_s} (m) \\otimes \\mathbf{G_s} (k))} - \\tau, 0\\right] Step 2d: Hypercomplex Cells (Second Competitive Stage) : In this stage, only the dominant orientation is preserved (winner-takes-all) for each location on the complex cell map, \\mathbf{C_s} (k) . \\mathbf{D}_s := \\max_k \\mathbf{D}_s (k) Step 2e: Multiple Scale Interaction: Boundary Localization and Noise Suppression : CONFIGR (Carpenter, Gaddam, and Mingolla, 2007) Paper Tolerance Space Theory (TST) for Gestalt Proximity Principle (Peng, Yang, and Li, 2021) Paper Gestalt Laws Models Border Ownership Models","title":"3. Segmentation and Grouping"},{"location":"pages/segmentation-and-grouping/#3-segmentation-and-grouping","text":"","title":"3. Segmentation and Grouping"},{"location":"pages/segmentation-and-grouping/#computer-vision-algorithms","text":"","title":"Computer Vision Algorithms"},{"location":"pages/segmentation-and-grouping/#region-growing","text":"","title":"Region Growing"},{"location":"pages/segmentation-and-grouping/#watershed","text":"","title":"Watershed"},{"location":"pages/segmentation-and-grouping/#k-means-clustering","text":"","title":"K-means Clustering"},{"location":"pages/segmentation-and-grouping/#mean-shift","text":"","title":"Mean Shift"},{"location":"pages/segmentation-and-grouping/#grabcut","text":"","title":"GrabCut"},{"location":"pages/segmentation-and-grouping/#u-net","text":"","title":"U-Net"},{"location":"pages/segmentation-and-grouping/#classical-region-proposal-methods","text":"","title":"Classical Region Proposal Methods"},{"location":"pages/segmentation-and-grouping/#neuroscience-models","text":"","title":"Neuroscience Models"},{"location":"pages/segmentation-and-grouping/#the-fbf-model","text":"Reference : Grossberg and Wyse, \"Figure-Ground Separation of Connected Scenic Figures: Boundaries, Filling-In, and Opponent Processing\", Neural Networks For Vision and Image Processing , Chapter 7, 1992. (Link to paper with the same topic but low scan quality) Overview : The FBF network model was used as part of the larger pattern recognition model by Grossberg et al.. It handles the initial steps of processing, up until the figure-ground separation phase. The FBF model is composed of two sub-systems: the Feature Contour System (FCS) and the Boundary Contour System (BCS), both initially proposed by Grossberg and Mingolla in 1985. As the processing sequence typically follows the pattern FCS-BCS-FCS, these combined systems are collectively referred to as the FBF networks. The central idea posited by Grossberg is that these two systems collaboratively enable us to recognize surfaces (handled by the FCS) and boundaries (handled by the BCS). The FCS's role is to fill in the entire boundary region with consistent surface characteristics, relying on the boundaries computed by the BCS, which connects collinear contours to establish these boundaries. A notable point raised by Grossberg is our ability to recognize boundaries even when they're not directly visible, leading to illusions such as the Kanizsa triangle. Please note that the FCS and BCS are hypothetical models for the visual system, distinguished by their function (\"what\" they are supposed to do) rather than their mechanism (\"how\" they are supposed to operate). These models have evolved significantly over time, and their definition is not bound by any specific method or implementation. The exact details often depend on specific research papers. Also, note that I've made a number of simplifications to the original model. For example, the original paper applied mathematics based on sub-pixel resolution (interpolating pixels), whereas the simple cells I used are entirely pixel-based. Algorithm Breakdown : The FBF model algorithm consists of four steps: Step 1: Discount the Illuminant : Instead of uniformly applying the ON-Center-OFF-Surround (ON-C) and OFF-Center-ON-Surround (OFF-C) kernels (also known as Difference of Gaussian), the kernel is scaled with the image value to handle areas with varying brightness levels. Grossberg describes these cells as \"shunting\" because their dynamics are dependent on their current state, akin to an electrical system with a (shunting) resistor. This is believed to be part of the FCS's work, as it needs to perceive surfaces as continuous over a broad range of illumination levels. The output of the ON-C and OFF-C processes are denoted as \\mathbf{x} and \\bar{\\mathbf{x}} respectively: \\mathbf{x} = \\frac{\\mathbf{I} \\otimes (B \\mathbf{C} - D \\mathbf{E})}{A + \\mathbf{I} \\otimes (\\mathbf{C} + \\mathbf{E})}\\\\ \\bar{\\mathbf{x}} = \\frac{A \\cdot S + \\mathbf{I} \\otimes (D \\mathbf{E} - B \\mathbf{C})}{A + \\mathbf{I} \\otimes (\\mathbf{C} + \\mathbf{E})} Here, the symbol \\otimes represents the operation of 2D cross-correlation. \\mathbf{I} represents the input image array, while A , B , and D are constants that define the shape of the ON-C and OFF-C kernels. The term S introduces the offset of the OFF-C kernel, ensuring that \\bar{\\mathbf{x}} primarily falls within the positive range. The values for those parameters can also be found in the paper. \\mathbf{C} and \\mathbf{E} denote two Gaussian kernels, as the ON-C and OFF-C kernels are \"Differences of Gaussians\" (DoG). The ON-C shape is derived from subtracting the wider and shorter Gaussian kernel \\mathbf{E} from the narrower and taller Gaussian kernel \\mathbf{C} . For the OFF-C kernel, the procedure is reversed. The formulas for these kernels are: \\mathbf{C} = C e^{-\\frac{(p - i)^2 + (q - j)^2}{\\alpha^2}}\\\\ \\mathbf{E} = E e^{-\\frac{(p - i)^2 + (q - j)^2}{\\beta^2}}\\\\ where \\alpha and \\beta are two other constants. (Interestingly, Grossberg and Wyse applied a logarithmic transformation to the arguments of the exponent in their paper, a step that I did not use in my implementation. Instead, I use the \"standard\" form of Gaussian shown above.) Step 2: CORT-X 2 Filter : The CORT-X 2 Filter is a neurophysiologically inspired model that draws on Hubel and Wiesel's work on Simple, Complex, and Hypercomplex cells. It forms the BCS part of this model. The filter takes the output from the previous step as its input, and generates the boundaries. It operates in a purely feedforward manner and can be broken down into six sub-steps: Step 2a: Simple Cells : These cells receive either \\mathbf{x} or \\bar{\\mathbf{x}} as inputs. The are two scales kernels s for the kernels: s=1 , with major and minor axes of 12 and 6 pixels, and s=2 , with major and minor axes of 20 and 10 pixels. The kernel, \\mathbf{K} , comprises two ellipse halves, \\mathbf{L_s} and \\mathbf{R_s} : \\mathbf{K}_{s, L} (k) = \\mathbf{L_s} (k) - \\alpha_s \\mathbf{R_s} (k) - \\beta_s \\mathbf{K}_{s, R} (k) = \\mathbf{R_s} (k) - \\alpha_s \\mathbf{L_s} (k) - \\beta_s Here, k denotes the orientation index of the kernel, ranging from k_0 = 0, k_1 = 22.5, ... to k_7 = 157.5 degrees. For clarity, note that Grossberg and Wyse didn't just rotate the simple cell kernel by 360 degrees. Instead, they rotate two kernels, \\mathbf{L_s} and \\mathbf{R_s} , with opposite polarities by 180 degrees. This is because the kernels of the same orientation and opposite polarity will combine in the next step, making this notation more intuitive. The output of the simple cells comes from the cross-correlation between the ON-C (or OFF-C) signals, followed by half-wave rectification: \\mathbf{S^+_{s, L}} (k) = \\max(\\mathbf{K}_{s, L} (k) \\otimes \\mathbf{x}, 0) \\mathbf{S^+_{s, R}} (k) = \\max(\\mathbf{K}_{s, R} (k) \\otimes \\mathbf{x}, 0) \\mathbf{S^-_{s, L}} (k) = \\max(\\mathbf{K}_{s, L} (k) \\otimes \\bar{\\mathbf{x}}, 0) \\mathbf{S^-_{s, R}} (k) = \\max(\\mathbf{K}_{s, R} (k) \\otimes \\bar{\\mathbf{x}}, 0) Step 2b: Complex Cells : The complex cells combine the responses from simple cells of the same orientation. Be aware that the \\mathbf{C} notation here represents the response map of complex cells, not the Gaussian from Step 1. \\mathbf{C}_s (k) = F \\left[ \\mathbf{S^+_{s, L}} (k) + \\mathbf{S^+_{s, R}} (k) + \\mathbf{S^-_{s, L}} (k) + \\mathbf{S^-_{s, R}} (k) \\right] Here, F is another constant parameter. Half-wave rectification isn't necessary as the output is always non-negative. Step 2c: Hypercomplex Cells (First Competitive Stage) : This step resembles the non-max suppression in Canny edge detection as it aims to inhibit texture around boundaries that lack other collinear edges. This is accomplished by dividing the complex cells response map by the cross-correlation between the complex cell response and the oriented competition kernel \\mathbf{G_s} (k) . This kernel is normalized and positive everywhere except along the direction of the complex cell's corresponding orientation. Consequently, if there are non-collinear edges in the vicinity, the cross-correlation term in the denominator becomes large. \\mathbf{D}_{s} (k) = \\max \\left[\\frac{\\mathbf{C_s}(k)}{\\epsilon + \\mu \\sum_m (\\mathbf{C_s} (m) \\otimes \\mathbf{G_s} (k))} - \\tau, 0\\right] Step 2d: Hypercomplex Cells (Second Competitive Stage) : In this stage, only the dominant orientation is preserved (winner-takes-all) for each location on the complex cell map, \\mathbf{C_s} (k) . \\mathbf{D}_s := \\max_k \\mathbf{D}_s (k) Step 2e: Multiple Scale Interaction: Boundary Localization and Noise Suppression :","title":"The FBF Model"},{"location":"pages/segmentation-and-grouping/#configr-carpenter-gaddam-and-mingolla-2007","text":"Paper","title":"CONFIGR (Carpenter, Gaddam, and Mingolla, 2007)"},{"location":"pages/segmentation-and-grouping/#tolerance-space-theory-tst-for-gestalt-proximity-principle-peng-yang-and-li-2021","text":"Paper","title":"Tolerance Space Theory (TST) for Gestalt Proximity Principle (Peng, Yang, and Li, 2021)"},{"location":"pages/segmentation-and-grouping/#gestalt-laws-models","text":"","title":"Gestalt Laws Models"},{"location":"pages/segmentation-and-grouping/#border-ownership-models","text":"","title":"Border Ownership Models"},{"location":"pages/stimulus/","text":"VisualAlgo::Stimulus Namespace Documentation The VisualAlgo::Stimulus namespace provides functions to create stimuli in the form of 2D matrices and add noise to them. Include #include \"helpers/Stimulus.hpp\" #include \"helpers/Matrix.hpp\" Functions Matrix random_mondrian(int rows, int cols, int num_rectangles); The type of stimuli were used in Edwin Land's research on illuminant discounting and filling-in (1977 and 1983). Land named it McCann Mondrians, a tribute to his research partner, John McCann, and the Dutch artist Piet Mondrian (Grossberg, 2021). Creates a new Matrix object with the given number of rows and cols , and draws a specified number of random overlapping rectangles in it. Each rectangle has a random position and size, and a random value in the range [0, 1]. The size of the rectangle scales with rows and cols . Returns the new Matrix . VisualAlgo::Stimulus::Matrix matrix = random_mondrian(10, 10, 10); void add_noise(Matrix &matrix, float noise_probability); Randomly changes some percentage of pixels in the given matrix to any value in the range [0, 1]. noise_probability is the fraction of pixels that will be changed. It must be a value between 0 and 1. VisualAlgo::Stimulus::Matrix matrix = random_mondrian(10, 10, 10); add_noise(matrix, 0.1); This creates a 10x10 matrix with 10 randomly placed squares, and then adds noise to 10% of the pixels.","title":"Stimulus"},{"location":"pages/stimulus/#visualalgostimulus-namespace-documentation","text":"The VisualAlgo::Stimulus namespace provides functions to create stimuli in the form of 2D matrices and add noise to them.","title":"VisualAlgo::Stimulus Namespace Documentation"},{"location":"pages/stimulus/#include","text":"#include \"helpers/Stimulus.hpp\" #include \"helpers/Matrix.hpp\"","title":"Include"},{"location":"pages/stimulus/#functions","text":"Matrix random_mondrian(int rows, int cols, int num_rectangles); The type of stimuli were used in Edwin Land's research on illuminant discounting and filling-in (1977 and 1983). Land named it McCann Mondrians, a tribute to his research partner, John McCann, and the Dutch artist Piet Mondrian (Grossberg, 2021). Creates a new Matrix object with the given number of rows and cols , and draws a specified number of random overlapping rectangles in it. Each rectangle has a random position and size, and a random value in the range [0, 1]. The size of the rectangle scales with rows and cols . Returns the new Matrix . VisualAlgo::Stimulus::Matrix matrix = random_mondrian(10, 10, 10); void add_noise(Matrix &matrix, float noise_probability); Randomly changes some percentage of pixels in the given matrix to any value in the range [0, 1]. noise_probability is the fraction of pixels that will be changed. It must be a value between 0 and 1. VisualAlgo::Stimulus::Matrix matrix = random_mondrian(10, 10, 10); add_noise(matrix, 0.1); This creates a 10x10 matrix with 10 randomly placed squares, and then adds noise to 10% of the pixels.","title":"Functions"}]}