<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Tony Fu" /><link rel="canonical" href="https://tonyfu97.github.io/VisualAlgo/pages/feature-extraction/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>2. Feature Extraction - VisualAlgo</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2. Feature Extraction";
        var mkdocs_page_input_path = "pages/feature-extraction.md";
        var mkdocs_page_url = "/VisualAlgo/pages/feature-extraction/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-274394082');
      </script>
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> VisualAlgo
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting-started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../matrix/">Matrix</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../stimulus/">Stimulus</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image-preprocessing-and-enhancement/">1. Image Pre-processing and Enhancement</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">2. Feature Extraction</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#computer-vision-algorithms">Computer Vision Algorithms</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#filters">Filters</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage">Example Usage</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gradients-class">Gradients Class</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#static-functions">Static Functions</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage_1">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#warning">Warning</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#visual-examples">Visual Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#edge-non-maximum-suppression">Edge Non-Maximum Suppression</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#class-methods">Class Methods</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage_2">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#note">Note</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#canny-edge-detection">Canny Edge Detection</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#class-members-and-methods">Class Members and Methods</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage_3">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#visual-examples_1">Visual Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#harris-corner-detection">Harris Corner Detection</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#class-members-and-methods_1">Class Members and Methods</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage_4">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#visual-examples_2">Visual Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#blob-detection-using-dog-and-log">Blob Detection using DoG and LoG</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#class-members-and-methods_2">Class Members and Methods</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage_5">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#visual-examples_3">Visual Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#sift">SIFT</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#surf">SURF</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#orb">ORB</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#hog">HOG</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#neuroscience-models">Neuroscience Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#simple-and-complex-cell-models">Simple and Complex Cell Models</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../segmentation-and-grouping/">3. Segmentation and Grouping</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../object-detection-and-recognition/">4. Object Detection and Recognition</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../motion-analysis-and-tracking/">5. Motion Analysis and Tracking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../depth-perception-and-3d-reconstruction/">6. Depth Perception and 3D Reconstruction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../color-vision/">7. Color Vision</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../attention-and-search/">8. Attention and Search</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../learning-and-memory/">9. Learning and Memory</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">VisualAlgo</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>2. Feature Extraction</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/tonyfu97/VisualAlgo/blob/master/docs/pages/feature-extraction.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2-feature-extraction">2. Feature Extraction</h1>
<h2 id="computer-vision-algorithms">Computer Vision Algorithms</h2>
<h3 id="filters">Filters</h3>
<p>In the <code>VisualAlgo::FeatureExtraction</code> namespace, a set of filter classes are provided for image processing tasks:</p>
<ul>
<li>
<p><code>Filter</code>: A base class with a pure virtual <code>apply</code> method for applying the filter to an image. </p>
</li>
<li>
<p><code>GaussianFilter</code>: A subclass of <code>Filter</code> that implements a Gaussian filter for image smoothing and noise reduction. It provides a constructor <code>GaussianFilter(float sigma)</code> to create a Gaussian filter with a specified sigma value, and overrides the <code>apply</code> method to perform Gaussian filtering on an image. The formula for the 2D Gaussian kernel is:</p>
</li>
</ul>
<p>
<script type="math/tex; mode=display">
g(x, y) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
</script>
</p>
<ul>
<li><code>SobelFilterX</code> and <code>SobelFilterY</code>: These are subclasses of <code>Filter</code> that implement the Sobel filter in the x and y directions respectively, used for edge detection and feature extraction tasks. The constructors <code>SobelFilterX()</code> and <code>SobelFilterY()</code> create the respective filters, and the <code>apply</code> method is overridden in each class to apply the corresponding Sobel filter on an image. The kernels are:</li>
</ul>
<p>
<script type="math/tex; mode=display">Sobel_x = \begin{bmatrix} -1 & 0 & 1 \\ -2 & 0 & 2 \\ -1 & 0 & 1 \end{bmatrix}</script>
</p>
<p>
<script type="math/tex; mode=display">Sobel_y = \begin{bmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix}</script>
</p>
<ul>
<li><code>LoGFilter</code>: The Laplacian of Gaussian (LoG) filter, used for edge detection and blob detection tasks. The constructor <code>LoGFilter(float sigma)</code> creates a LoG filter with a specified sigma value, and overrides the <code>apply</code> method to perform LoG filtering on an image. The Laplacian of Gaussian is defined as the second derivative of the Gaussian, so applying the LoG operation to an image corresponds to smoothing the image with a Gaussian filter and then finding the second derivative of the result. The formula for the 2D LoG kernel is:</li>
</ul>
<p>
<script type="math/tex; mode=display">
g(x, y) = \frac{(x^2 + y^2 - 2\sigma^2)}{2\pi\sigma^4} \exp\left(-\frac{x^2 + y^2}{2\sigma^2}\right)
</script>
</p>
<h4 id="example-usage">Example Usage</h4>
<p>In this example, the <code>GaussianFilter</code>, <code>SobelFilterX</code>, and <code>SobelFilterY</code> classes are used to apply corresponding filters to an image. The filtered images are then saved for later analysis or visualization.</p>
<pre><code class="language-cpp">#include &quot;FeatureExtraction/Filter.hpp&quot;
#include &quot;helpers/Matrix.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);
image.normalize();

VisualAlgo::FeatureExtraction::GaussianFilter gaussianFilter(0.8f);
VisualAlgo::FeatureExtraction::SobelFilterX sobelXFilter;
VisualAlgo::FeatureExtraction::SobelFilterY sobelYFilter;

VisualAlgo::Matrix image_gaussian, image_sobel_x, image_sobel_y;
image_gaussian = gaussianFilter.apply(image);
image_sobel_x = sobelXFilter.apply(image);
image_sobel_y = sobelYFilter.apply(image);

image_gaussian.save(&quot;datasets/FeatureExtraction/cat_gaussian.ppm&quot;, true);
image_sobel_x.save(&quot;datasets/FeatureExtraction/cat_sobel_x.ppm&quot;, true);
image_sobel_y.save(&quot;datasets/FeatureExtraction/cat_sobel_y.ppm&quot;, true);
</code></pre>
<hr />
<h3 id="gradients-class">Gradients Class</h3>
<p>The <code>Gradients</code> class in the <code>VisualAlgo::FeatureExtraction</code> namespace is a utility class for computing the x and y gradients of an image, which are important components in various computer vision and image processing tasks such as edge detection and feature extraction. </p>
<h4 id="static-functions">Static Functions</h4>
<ul>
<li>
<p><code>Matrix computeXGradient(const Matrix&amp; image)</code>: This function computes the x-direction gradients of the image using a Sobel filter.</p>
</li>
<li>
<p><code>Matrix computeYGradient(const Matrix&amp; image)</code>: This function computes the y-direction gradients of the image using a Sobel filter.</p>
</li>
<li>
<p><code>Matrix computeGradientMagnitude(const Matrix&amp; xGradient, const Matrix&amp; yGradient)</code>: This function computes the magnitude of the gradient at each pixel, defined as the square root of the sum of the squares of the x and y gradients. The output is a <code>Matrix</code> where each element represents the gradient magnitude at the corresponding pixel.</p>
</li>
<li>
<p><code>Matrix computeGradientDirection(const Matrix&amp; xGradient, const Matrix&amp; yGradient, float threshold = 0.01)</code>: This function computes the direction of the gradient at each pixel, defined as the arctangent of the ratio of the y-gradient to the x-gradient. It accepts a threshold parameter to filter out low magnitude gradients, reducing the noise. Any gradient with a magnitude less than the threshold will be set to zero in the output matrix.</p>
</li>
</ul>
<h4 id="example-usage_1">Example Usage</h4>
<p>In this example, the <code>Gradients</code> class is used to compute the x and y gradients of an image. These gradients are then saved to file and compared with the expected gradients to ensure the computations are correct.</p>
<pre><code class="language-cpp">#include &quot;FeatureExtraction/Gradients.hpp&quot;
#include &quot;helpers/Matrix.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);
image.normalize();

VisualAlgo::Matrix image_x_gradient, image_y_gradient;
image_x_gradient = VisualAlgo::FeatureExtraction::Gradients::computeXGradient(image);
image_y_gradient = VisualAlgo::FeatureExtraction::Gradients::computeYGradient(image);

VisualAlgo::Matrix gradient_direction;
gradient_direction = VisualAlgo::FeatureExtraction::Gradients::computeGradientDirection(image_x_gradient, image_y_gradient, 0.01);

image_x_gradient.save(&quot;datasets/FeatureExtraction/cat_x_gradient.ppm&quot;, true);
image_y_gradient.save(&quot;datasets/FeatureExtraction/cat_y_gradient.ppm&quot;, true);
gradient_direction.save(&quot;datasets/FeatureExtraction/cat_gradient_direction.ppm&quot;, true);
</code></pre>
<p>In this code, <code>computeXGradient</code> and <code>computeYGradient</code> are used to calculate the x and y gradients of the loaded image respectively. The <code>computeGradientDirection</code> function is then used to calculate the gradient direction of the image, with a threshold of 0.01 for filtering out low magnitude gradients. The resulting gradient direction image is then saved to file for later analysis or visualization.</p>
<h4 id="warning">Warning</h4>
<p>The gradient direction computation involves taking the arctangent of the ratio of the y-gradient to the x-gradient. In areas of the image where the x-gradient is near zero, this ratio can become very large or very small, leading to potential instability in the arctangent calculation. To mitigate this, we provide an option to set a threshold, under which the gradient magnitude is deemed insignificant and the direction is set to zero, reducing the influence of noise or small variations in uniform areas of the image. However, care should be taken when setting the threshold as a very high value might discard relevant information while a very low value might not effectively filter out the noise.</p>
<h4 id="visual-examples">Visual Examples</h4>
<p>Below are visual examples of the original image and the computed gradients, gradient magnitude, and gradient direction.</p>
<p>Original Image:</p>
<p><img alt="original_lighthouse" src="../../images/FeatureExtraction/lighthouse_original.png" /></p>
<p>X-direction Gradient:</p>
<p><img alt="sobel_x_lighthouse" src="../../images/FeatureExtraction/lighthouse_sobel_x.png" /></p>
<p>Y-direction Gradient:</p>
<p><img alt="sobel_y_lighthouse" src="../../images/FeatureExtraction/lighthouse_sobel_y.png" /></p>
<p>Gradient Magnitude:</p>
<p><img alt="sobel_mag_lighthouse" src="../../images/FeatureExtraction/lighthouse_sobel_magnitude.png" /></p>
<p>Gradient Direction:</p>
<p><img alt="sobel_dir_lighthouse" src="../../images/FeatureExtraction/lighthouse_sobel_direction.png" /></p>
<hr />
<h3 id="edge-non-maximum-suppression">Edge Non-Maximum Suppression</h3>
<p>The <code>EdgeNonMaxSuppression</code> class in the <code>VisualAlgo::FeatureExtraction</code> namespace is used for edge thinning in an image. It operates by suppressing all the non-maximum edges in the computed gradient of the image, leading to thin edges in the output. This is a key step in several edge detection algorithms such as the Canny edge detector.</p>
<p><img alt="edge_non_max_suppression" src="../../images/FeatureExtraction/edge_non_max_suppression.jpg" /></p>
<h4 id="class-methods">Class Methods</h4>
<ul>
<li>
<p><code>Matrix apply(const Matrix &amp;image)</code>: This method takes as input a <code>Matrix</code> representing an image and applies edge non-maximum suppression to it. It first computes the x and y gradients of the image, then the gradient magnitude and direction using the <code>Gradients</code> class. After that, it calls the other <code>apply</code> method with the computed gradient magnitude and direction as arguments.</p>
</li>
<li>
<p><code>Matrix apply(const Matrix &amp;gradientMagnitude, const Matrix &amp;gradientDirection)</code>: This method takes as input a <code>Matrix</code> each representing the gradient magnitude and direction of an image. It then applies edge non-maximum suppression to it. For each pixel, it rounds the gradient direction to one of four possible directions, then compares the gradient magnitude of the current pixel with its two neighbors in the direction of the gradient. If the gradient magnitude of the current pixel is greater than both of its neighbors, it is preserved in the output; otherwise, it is suppressed.</p>
</li>
</ul>
<h4 id="example-usage_2">Example Usage</h4>
<p>In this example, the <code>EdgeNonMaxSuppression</code> class is used to apply edge non-maximum suppression to an image. The image is first loaded and normalized. The <code>apply</code> function of the <code>EdgeNonMaxSuppression</code> class is then called with the image as argument, and the resulting edge-thinned image is saved to file.</p>
<pre><code class="language-cpp">#include &quot;FeatureExtraction/EdgeNonMaxSuppression.hpp&quot;
#include &quot;helpers/Matrix.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);
image.normalize();

VisualAlgo::FeatureExtraction::EdgeNonMaxSuppression edgeNonMaxSuppression;
VisualAlgo::Matrix edge_thinned_image;
edge_thinned_image = edgeNonMaxSuppression.apply(image);

edge_thinned_image.save(&quot;datasets/FeatureExtraction/cat_edge_non_max_suppression.ppm&quot;, true);
</code></pre>
<h4 id="note">Note</h4>
<p>The implementation assumes that the image has already been smoothed to remove noise and that appropriate gradient magnitude and direction have been computed. The edge non-maximum suppression is then used to thin out the edges in the image.</p>
<hr />
<h3 id="canny-edge-detection">Canny Edge Detection</h3>
<p>The <code>Canny</code> class in the <code>VisualAlgo::FeatureExtraction</code> namespace is an implementation of the Canny edge detection algorithm. The Canny edge detection algorithm, developed by John F. Canny in 1986, is a multi-stage algorithm used to detect a wide range of edges in images. The Canny algorithm involves several stages:</p>
<ol>
<li>
<p><strong>Noise Reduction</strong>: Since edge detection is susceptible to noise in an image, the first step is to remove the noise. This is typically done using a Gaussian filter.</p>
</li>
<li>
<p><strong>Gradient Calculation</strong>: The edge in an image is the area where there is a sharp change in the color or intensity of the image. The Gradient calculation step measures this change in the x and y direction.</p>
</li>
<li>
<p><strong>Non-maximum Suppression</strong>: The Gradient calculation process results in thick edges. The purpose of non-maximum suppression is to convert these thick edges into thin lines.</p>
</li>
<li>
<p><strong>Double Threshold</strong>: Potential edges are determined by thresholding the remaining pixels based on their gradient value. This results in strong edges, weak edges, and non-edges.</p>
</li>
<li>
<p><strong>Edge Tracking by Hysteresis</strong>: Weak edges are pruned based on their connectivity. If a weak edge is connected to a strong edge, it is considered part of an edge. Otherwise, it is discarded.</p>
</li>
</ol>
<h4 id="class-members-and-methods">Class Members and Methods</h4>
<ul>
<li>
<p><code>Canny(float sigma, float low_threshold, float high_threshold)</code>: Constructor that initializes a <code>Canny</code> instance with the specified sigma value for the Gaussian filter, and low and high threshold values for edge detection.</p>
</li>
<li>
<p><code>Matrix apply(const Matrix &amp;image)</code>: Applies the Canny edge detection algorithm to an input image. This function first applies Gaussian filtering to the input image for smoothing and noise reduction, then computes the gradients of the smoothed image. After that, it applies non-maximum suppression to the gradient magnitude of the image to thin the edges. Finally, it applies thresholding and edge tracking to detect the edges in the image.</p>
</li>
<li>
<p><code>Matrix applyThreshold(const Matrix &amp;image)</code>: A private method that applies thresholding to an image based on the low and high threshold values. This is used to detect potential edges in the image.</p>
</li>
<li>
<p><code>Matrix trackEdges(const Matrix &amp;image)</code>: A private method that tracks edges in an image using hysteresis thresholding. This is used to finalize the detected edges in the image.</p>
</li>
</ul>
<h4 id="example-usage_3">Example Usage</h4>
<p>In this example, the <code>Canny</code> class is used to apply the Canny edge detection algorithm to a cat image.</p>
<pre><code class="language-cpp">#include &quot;helpers/Matrix.hpp&quot;
#include &quot;FeatureExtraction/Canny.hpp&quot;

VisualAlgo::Matrix image, image_canny_expected;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);

VisualAlgo::FeatureExtraction::Canny canny(1.0f, 0.1f, 0.2f);
VisualAlgo::Matrix image_canny = canny.apply(image);

image_canny.save(&quot;datasets/FeatureExtraction/cat_canny.ppm&quot;, true);
</code></pre>
<h4 id="visual-examples_1">Visual Examples</h4>
<p>Below are visual examples of the original image and the computed Canny edges.</p>
<p>Original Image:</p>
<p><img alt="original_lighthouse" src="../../images/FeatureExtraction/lighthouse_original.png" /></p>
<p>Canny Edges (sigma = 1.0, low_threshold = 0.1, high_threshold = 0.2):</p>
<p><img alt="canny_lighthouse" src="../../images/FeatureExtraction/lighthouse_canny.png" /></p>
<hr />
<h3 id="harris-corner-detection">Harris Corner Detection</h3>
<p>The <code>Harris</code> class in the <code>VisualAlgo::FeatureExtraction</code> namespace provides an implementation of the Harris Corner Detection algorithm. This algorithm, introduced by Chris Harris and Mike Stephens in 1988, is a corner detection operator that identifies corners and edge junctions in images. It is effective due to its invariance to rotation, scale, and illumination changes. </p>
<p>The Harris Corner Detection algorithm has several stages:</p>
<ol>
<li>
<p><strong>Gradient Calculation</strong>: The algorithm begins by calculating the gradient images Ix and Iy. These are obtained by convolving the original image with a derivative of Gaussian filter, providing a measure of intensity change in both the x and y directions.</p>
</li>
<li>
<p><strong>Components of the Structure Tensor</strong>: The algorithm then calculates three images, each corresponding to the components of the structure tensor (also known as the second moment matrix) for each pixel. These images are <script type="math/tex">I_x^2</script>, <script type="math/tex">I_y^2</script>, and <script type="math/tex">I_x \cdot I_y</script>, representing the gradient squared in each direction and the product of the gradients, respectively.</p>
</li>
<li>
<p><strong>Gaussian smoothing</strong>: Next, the images obtained from the previous step are convolved with a Gaussian filter. This smoothing process allows for the aggregation of the squared and product of gradients over a certain neighborhood, leading to the images <script type="math/tex">S_{xx}</script>, <script type="math/tex">S_{yy}</script>, and <script type="math/tex">S_{xy}</script>.</p>
</li>
<li>
<p><strong>Corner Response Calculation</strong>: The corner response matrix, <script type="math/tex">R</script>, is calculated for each pixel in the image using the formula <script type="math/tex">R = \det(M) - k*(\text{trace}(M))^2</script>, where M is the structure tensor, or second moment matrix. <script type="math/tex">M</script> is a <script type="math/tex">2 \times 2</script> matrix defined as follows:</p>
<p>
<script type="math/tex; mode=display">
M = \begin{bmatrix} S_{xx} & S_{xy} \\ S_{xy} & S_{yy} \end{bmatrix}
</script>
</p>
<p>The determinant of <script type="math/tex">M</script> (<script type="math/tex">\det(M)</script>) and the trace of <script type="math/tex">M</script> (<script type="math/tex">\text{trace}(M)</script>) are computed as follows:</p>
<p>
<script type="math/tex; mode=display">\det(M) = S_{xx} \cdot S_{yy} - S_{xy} \cdot S_{xy}</script>
</p>
<p>
<script type="math/tex; mode=display">\text{trace}(M) = S_{xx} + S_{yy}</script>
</p>
<p>The structure tensor <script type="math/tex">M</script> plays a key role in feature detection as it represents the distribution of gradients within a specific neighborhood around a point. Rather than directly comparing the gradient of a pixel with those of its neighbors, we use a Gaussian function to calculate an average gradient across an area.</p>
<p>In essence, the structure tensor captures the underlying geometric structure in the vicinity of each pixel. It accomplishes this by portraying gradient orientations as an ellipse in the (<script type="math/tex">I_x, I_y</script>) plane within a specific window. Here, the determinant is directly proportional to the area of the ellipse, while the trace is equivalent to the sum of the lengths of the ellipse's major and minor axes.</p>
<ul>
<li>
<p>Presence of an edge: When an image contains an edge, the distribution of gradients forms a slender, elongated ellipse. This happens because the intensity changes consistently in one direction (along the edge) and shows little to no change in the direction perpendicular to it. The major axis of this ellipse aligns with the direction of the edge.</p>
</li>
<li>
<p>Presence of a corner: If a corner is present, the gradients are distributed more evenly, resulting in an elliptical shape that resembles a circle. This is because a corner features significant intensity changes in multiple directions.</p>
</li>
<li>
<p>Flat region: In a flat region of the image, where there is minimal change in intensity in any direction, the ellipse is small, signaling the absence of distinctive features.</p>
</li>
</ul>
</li>
<li>
<p><strong>Thresholding</strong>: The final step involves applying a threshold value to the corner response matrix, <script type="math/tex">R</script>. Positions in the image that correspond to R values above the threshold are considered corners. The output is an image with highlighted positions where corners exist. </p>
</li>
</ol>
<h4 id="class-members-and-methods_1">Class Members and Methods</h4>
<ul>
<li>
<p><code>Harris(float sigma, float k, float threshold)</code>: Constructor that initializes a <code>Harris</code> instance with the specified sigma value for the Gaussian filter, a k value used in the formula for the response <script type="math/tex">R</script>, and a threshold value for detecting corners.</p>
</li>
<li>
<p><code>Matrix apply(const Matrix &amp;image) const</code>: Applies the Harris Corner Detection algorith to an input image.</p>
</li>
<li>
<p><code>std::vector&lt;std::pair&lt;int, int&gt;&gt; detect(const Matrix &amp;image)</code>: Applies the Harris Corner Detection algorithm to an input image. The <code>apply</code> method above actually calls this method.</p>
</li>
</ul>
<h4 id="example-usage_4">Example Usage</h4>
<p>In this example, the <code>Harris</code> class is used to apply the Harris Corner Detection algorithm to a cat image.</p>
<pre><code class="language-cpp">#include &quot;helpers/Matrix.hpp&quot;
#include &quot;FeatureExtraction/Harris.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);
image.normalize();

VisualAlgo::FeatureExtraction::Harris harris(1.0f, 0.04f, 0.2f);
VisualAlgo::Matrix image_harris = harris.apply(image);

image_harris.save(&quot;datasets/FeatureExtraction/cat_harris.ppm&quot;, true);
</code></pre>
<h4 id="visual-examples_2">Visual Examples</h4>
<p>Below are visual examples of the original image and the detected corners.</p>
<p>Original Image:</p>
<p><img alt="original_cat" src="../../images/FeatureExtraction/lighthouse_original.png" /></p>
<p>Detected Corners (sigma = 1.0, k = 0.04, threshold = 0.2):</p>
<p><img alt="harris_cat" src="../../images/FeatureExtraction/lighthouse_harris.png" /></p>
<hr />
<h3 id="blob-detection-using-dog-and-log">Blob Detection using DoG and LoG</h3>
<p>The <code>BlobDoG</code> and <code>BlobLoG</code> classes in the <code>VisualAlgo::FeatureExtraction</code> namespace implement blob detection algorithms using Difference of Gaussians (DoG) and Laplacian of Gaussians (LoG) respectively. In image processing, a "blob" refers to a group of pixels within an image that share specific characteristics, forming a distinct region of interest. Notably, detected blobs may not always resemble what we typically visualize as a blob, since their identification relies more on algorithmic criteria rather than human visual perception.</p>
<p>The input image is processed with a Gaussian filter (for DoG) or a Laplacian of Gaussian filter (for LoG) at different scales, generating what we refer to as "scale-space" - a 3D representation. Subsequently, a <strong>3D</strong> window is used to locate the local maxima. (Yes, this comparison is not conducted across the entire scale but within a windowed range of scales.)</p>
<h4 id="class-members-and-methods_2">Class Members and Methods</h4>
<ul>
<li>
<p><code>BlobDoG(float initial_sigma, float k, float threshold, int window_size, int octaves)</code> and <code>BlobLoG(float initial_sigma, float k, float threshold, int window_size, int octaves)</code>: Constructors that initialize a <code>BlobDoG</code> or <code>BlobLoG</code> instance with the specified parameters. </p>
<ul>
<li>
<p><code>initial_sigma</code>: the initial standard deviation for the Gaussian filter. It determines the size of the smallest scale (or blob) that can be detected. It should be a positive value.</p>
</li>
<li>
<p><code>k</code>: the scale multiplication factor. It determines the factor by which the scale increases for each subsequent layer in an octave. It can be any real number other than 1. If <code>k</code> is greater than 1, the scales increase in size, and if <code>k</code> is less than 1, the scales decrease in size.</p>
</li>
<li>
<p><code>threshold</code>: the minimum intensity difference to consider for a point to be a local maximum in the scale-space. It should be a positive value.</p>
</li>
<li>
<p><code>window_size</code>: the size of the window used for local maxima detection in the scale-space. It needs to be an odd, positive integer and determines the size of the 3D neighborhood (x, y, and scale) within which the local maxima are searched. This helps in detecting blobs of varying sizes.</p>
</li>
<li>
<p><code>octaves</code>: the number of octaves to be used in the scale-space representation. An octave in this context represents a series of scale-space layers where the scale doubles from beginning to end. It should be a positive integer.</p>
</li>
</ul>
</li>
<li>
<p><code>Matrix apply(const Matrix &amp;image)</code>: Applies the blob detection algorithm to an input image and returns a Matrix where the blobs are highlighted.</p>
</li>
<li>
<p><code>std::vector&lt;std::tuple&lt;int, int, float&gt;&gt; detect(const Matrix &amp;image)</code>: Returns a vector of tuples indicating the detected blobs. Each tuple contains the row and column of the detected blob and the sigma value at which the blob was detected.</p>
</li>
</ul>
<h4 id="example-usage_5">Example Usage</h4>
<p>In this example, the <code>BlobDoG</code> and <code>BlobLoG</code> classes are used to apply the blob detection algorithms to an image.</p>
<pre><code class="language-cpp">#include &quot;helpers/Matrix.hpp&quot;
#include &quot;FeatureExtraction/Blob.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/FeatureExtraction/cat_resized.ppm&quot;);

VisualAlgo::FeatureExtraction::BlobDoG blobDoG(1.0f, 1.6f, 0.03f, 3, 4);
VisualAlgo::Matrix image_blobs_DoG = blobDoG.apply(image);

image_blobs_DoG.save(&quot;datasets/FeatureExtraction/cat_blobs_DoG.ppm&quot;, true);

VisualAlgo::FeatureExtraction::BlobLoG blobLoG(1.0f, 1.6f, 0.03f, 3, 4);
VisualAlgo::Matrix image_blobs_LoG = blobLoG.apply(image);

image_blobs_LoG.save(&quot;datasets/FeatureExtraction/cat_blobs_LoG.ppm&quot;, true);
</code></pre>
<h4 id="visual-examples_3">Visual Examples</h4>
<p>Below are visual examples of the original image and the detected blobs using DoG and LoG.</p>
<p>Original Image:</p>
<p><img alt="original_mondrian" src="../../images/FeatureExtraction/mondrian_original.png" /></p>
<p>Blobs using DoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1):</p>
<p><img alt="DoG_mondrian" src="../../images/FeatureExtraction/mondrian_blob_dog.png" /></p>
<p>Blobs using LoG (initial_sigma = 10.0, k = 0.7, threshold = 0.2, window_size = 5, octaves = 1):</p>
<p><img alt="LoG_mondrian" src="../../images/FeatureExtraction/mondrian_blob_log.png" /></p>
<hr />
<h3 id="sift">SIFT</h3>
<hr />
<h3 id="surf">SURF</h3>
<hr />
<h3 id="orb">ORB</h3>
<hr />
<h3 id="hog">HOG</h3>
<hr />
<h2 id="neuroscience-models">Neuroscience Models</h2>
<h3 id="simple-and-complex-cell-models">Simple and Complex Cell Models</h3>
<hr />
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../image-preprocessing-and-enhancement/" class="btn btn-neutral float-left" title="1. Image Pre-processing and Enhancement"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../segmentation-and-grouping/" class="btn btn-neutral float-right" title="3. Segmentation and Grouping">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright &copy; 2023 Tony Fu</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/tonyfu97/VisualAlgo/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../image-preprocessing-and-enhancement/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../segmentation-and-grouping/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
