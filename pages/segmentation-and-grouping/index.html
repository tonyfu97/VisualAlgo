<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Tony Fu" /><link rel="canonical" href="https://tonyfu97.github.io/VisualAlgo/pages/segmentation-and-grouping/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>3. Segmentation and Grouping - VisualAlgo</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "3. Segmentation and Grouping";
        var mkdocs_page_input_path = "pages/segmentation-and-grouping.md";
        var mkdocs_page_url = "/VisualAlgo/pages/segmentation-and-grouping/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/languages/django.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-274394082"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-274394082');
      </script>
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> VisualAlgo
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting-started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../matrix/">Matrix</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../stimulus/">Stimulus</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../image-preprocessing-and-enhancement/">1. Image Pre-processing and Enhancement</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../feature-extraction/">2. Feature Extraction</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">3. Segmentation and Grouping</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#computer-vision-algorithms">Computer Vision Algorithms</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#thresholding">Thresholding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#class-members-and-methods">Class Members and Methods</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#example-usage">Example Usage</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#visual-examples">Visual Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#region-growing">Region Growing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#watershed">Watershed</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#k-means-clustering">K-means Clustering</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mean-shift">Mean Shift</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#grabcut">GrabCut</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#u-net">U-Net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#classical-region-proposal-methods">Classical Region Proposal Methods</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#neuroscience-models">Neuroscience Models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#the-fbf-model">The FBF Model</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#configr-carpenter-gaddam-and-mingolla-2007">CONFIGR (Carpenter, Gaddam, and Mingolla, 2007)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#tolerance-space-theory-tst-for-gestalt-proximity-principle-peng-yang-and-li-2021">Tolerance Space Theory (TST) for Gestalt Proximity Principle (Peng, Yang, and Li, 2021)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gestalt-laws-models">Gestalt Laws Models</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#border-ownership-models">Border Ownership Models</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../object-detection-and-recognition/">4. Object Detection and Recognition</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../motion-analysis-and-tracking/">5. Motion Analysis and Tracking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../depth-perception-and-3d-reconstruction/">6. Depth Perception and 3D Reconstruction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../color-vision/">7. Color Vision</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../attention-and-search/">8. Attention and Search</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../learning-and-memory/">9. Learning and Memory</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">VisualAlgo</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>3. Segmentation and Grouping</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/tonyfu97/VisualAlgo/blob/master/docs/pages/segmentation-and-grouping.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="3-segmentation-and-grouping">3. Segmentation and Grouping</h1>
<h2 id="computer-vision-algorithms">Computer Vision Algorithms</h2>
<h3 id="thresholding">Thresholding</h3>
<p>The <code>Thresholding</code> class in the <code>VisualAlgo::SegmentationAndGrouping</code> namespace is the image thresholding algorithm. Image thresholding is a simple method of segmenting an image into different regions. It separates an image into foreground and background, by turning all pixels below some threshold to zero and all pixels above that threshold to one (or 255 in an 8-bit grayscale image).</p>
<h4 id="class-members-and-methods">Class Members and Methods</h4>
<ul>
<li><code>apply(const Matrix &amp;img, float threshold)</code>: A static method that applies binary thresholding to the input image based on the given threshold value. All pixel intensity values below the threshold are set to 0 (representing the background), and all pixel intensity values equal to or above the threshold are set to 1 (representing the foreground).</li>
</ul>
<h4 id="example-usage">Example Usage</h4>
<p>In this example, the <code>Thresholding</code> class is used to apply binary thresholding to a coins image.</p>
<pre><code class="language-cpp">#include &quot;helpers/Matrix.hpp&quot;
#include &quot;SegmentationAndGrouping/Thresholding.hpp&quot;

VisualAlgo::Matrix image;
image.load(&quot;datasets/SegmentationAndGrouping/coins.png&quot;);

VisualAlgo::Matrix threshold_image = VisualAlgo::SegmentationAndGrouping::Thresholding::apply(image, 0.5f);

threshold_image.save(&quot;datasets/SegmentationAndGrouping/coins_thresholding.png&quot;, true);
</code></pre>
<h4 id="visual-examples">Visual Examples</h4>
<p>Here are some pictures to show the steps of the process.</p>
<p>Original Image:</p>
<p><img alt="coins_original" src="../../images/SegmentationAndGrouping/coins.png" /></p>
<p>Image After Thresholding:</p>
<p><img alt="coins_threshold" src="../../images/SegmentationAndGrouping/coins_thresholding.png" /></p>
<p>In the image above, after we used thresholding, you can see it looks grainy. This is because of the small bits of noise in the original picture. To clean this up without blurring the edges of the coins too much, we can use a median filter first, then apply thresholding.</p>
<p>Image After Median Filter and Thresholding:</p>
<p><img alt="coins_median_threshold" src="../../images/SegmentationAndGrouping/coins_median_thresholding.png" /></p>
<p>After applying thresholding to the coin image, we might see small bits of noise again. We can clean this up by applying another median filter.</p>
<p>Image After Median Filter, Thresholding, and Another Median Filter:</p>
<p><img alt="coins_median_threshold_median" src="../../images/SegmentationAndGrouping/coins_median_thresholding_median.png" /></p>
<hr />
<h3 id="region-growing">Region Growing</h3>
<hr />
<h3 id="watershed">Watershed</h3>
<hr />
<h3 id="k-means-clustering">K-means Clustering</h3>
<hr />
<h3 id="mean-shift">Mean Shift</h3>
<hr />
<h3 id="grabcut">GrabCut</h3>
<hr />
<h3 id="u-net">U-Net</h3>
<hr />
<h3 id="classical-region-proposal-methods">Classical Region Proposal Methods</h3>
<hr />
<h2 id="neuroscience-models">Neuroscience Models</h2>
<h3 id="the-fbf-model">The FBF Model</h3>
<p><strong>Reference</strong>: Grossberg and Wyse, "Figure-Ground Separation of Connected Scenic Figures: Boundaries, Filling-In, and Opponent Processing", <em>Neural Networks For Vision and Image Processing</em>, Chapter 7, 1992. <a href="https://sites.bu.edu/steveg/files/2016/06/GroWyse1991NN.pdf">(Link to paper with the same topic but low scan quality)</a></p>
<p><strong>Overview</strong>: </p>
<p>The FBF network model was used as part of the larger pattern recognition model by Grossberg et al.. It handles the initial steps of processing, up until the figure-ground separation phase. </p>
<p>The FBF model is composed of two sub-systems: the Feature Contour System (FCS) and the Boundary Contour System (BCS), both initially proposed by Grossberg and Mingolla in 1985. As the processing sequence typically follows the pattern FCS-BCS-FCS, these combined systems are collectively referred to as the FBF networks.</p>
<p>The central idea posited by Grossberg is that these two systems collaboratively enable us to recognize surfaces (handled by the FCS) and boundaries (handled by the BCS). The FCS's role is to fill in the entire boundary region with consistent surface characteristics, relying on the boundaries computed by the BCS, which connects collinear contours to establish these boundaries. A notable point raised by Grossberg is our ability to recognize boundaries even when they're not directly visible, leading to illusions such as the Kanizsa triangle.</p>
<p>Please note that the FCS and BCS are hypothetical models for the visual system, distinguished by their function ("what" they are supposed to do) rather than their mechanism ("how" they are supposed to operate). These models have evolved significantly over time, and their definition is not bound by any specific method or implementation. The exact details often depend on specific research papers. Also, note that I've made a number of simplifications to the original model. For example, the original paper applied mathematics based on sub-pixel resolution (interpolating pixels), whereas the simple cells I used are entirely pixel-based.</p>
<p><strong>Algorithm Breakdown</strong>: </p>
<p>The FBF model algorithm consists of four steps:</p>
<ul>
<li>
<p><strong>Step 1: Discount the Illuminant</strong>: Instead of uniformly applying the ON-Center-OFF-Surround (ON-C) and OFF-Center-ON-Surround (OFF-C) kernels (also known as Difference of Gaussian), the kernel is scaled with the image value to handle areas with varying brightness levels. Grossberg describes these cells as "shunting" because their dynamics are dependent on their current state, akin to an electrical system with a (shunting) resistor. This is believed to be part of the FCS's work, as it needs to perceive surfaces as continuous over a broad range of illumination levels. The output of the ON-C and OFF-C processes are denoted as <script type="math/tex">\mathbf{x}</script> and <script type="math/tex">\bar{\mathbf{x}}</script> respectively:</p>
<p>
<script type="math/tex; mode=display"> \mathbf{x} = \frac{\mathbf{I} \otimes (B \mathbf{C} - D \mathbf{E})}{A + \mathbf{I} \otimes (\mathbf{C} + \mathbf{E})}\\
\bar{\mathbf{x}} = \frac{A \cdot S + \mathbf{I} \otimes (D \mathbf{E} - B \mathbf{C})}{A + \mathbf{I} \otimes (\mathbf{C} + \mathbf{E})}</script>
</p>
<p>Here, the symbol <script type="math/tex">\otimes</script> represents the operation of 2D cross-correlation. <script type="math/tex">\mathbf{I}</script> represents the input image array, while <script type="math/tex">A</script>, <script type="math/tex">B</script>, and <script type="math/tex">D</script> are constants that define the shape of the ON-C and OFF-C kernels. The term <script type="math/tex">S</script> introduces the offset of the OFF-C kernel, ensuring that <script type="math/tex">\bar{\mathbf{x}}</script> primarily falls within the positive range. The values for those parameters can also be found in the paper.</p>
<p>
<script type="math/tex">\mathbf{C}</script> and <script type="math/tex">\mathbf{E}</script> denote two Gaussian kernels, as the ON-C and OFF-C kernels are "Differences of Gaussians" (DoG). The ON-C shape is derived from subtracting the wider and shorter Gaussian kernel <script type="math/tex">\mathbf{E}</script> from the narrower and taller Gaussian kernel <script type="math/tex">\mathbf{C}</script>. For the OFF-C kernel, the procedure is reversed. The formulas for these kernels are:</p>
<p>
<script type="math/tex; mode=display">
\mathbf{C} = C e^{-\frac{(p - i)^2 + (q - j)^2}{\alpha^2}}\\
\mathbf{E} = E e^{-\frac{(p - i)^2 + (q - j)^2}{\beta^2}}\\
</script>
</p>
<p>where <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> are two other constants. (Interestingly, Grossberg and Wyse applied a logarithmic transformation to the arguments of the exponent in their paper, a step that I did not use in my implementation. Instead, I use the "standard" form of Gaussian shown above.)</p>
</li>
<li>
<p><strong>Step 2: CORT-X 2 Filter</strong>: The CORT-X 2 Filter is a neurophysiologically inspired model that draws on Hubel and Wiesel's work on Simple, Complex, and Hypercomplex cells. It forms the BCS part of this model. The filter takes the output from the previous step as its input, and generates the boundaries. It operates in a purely feedforward manner and can be broken down into six sub-steps:</p>
<ul>
<li>
<p><strong>Step 2a: Simple Cells</strong>: These cells receive either <script type="math/tex">\mathbf{x}</script> or <script type="math/tex">\bar{\mathbf{x}}</script> as inputs. The are two scales kernels <script type="math/tex">s</script> for the kernels: <script type="math/tex">s=1</script>, with major and minor axes of 12 and 6 pixels, and <script type="math/tex">s=2</script>, with major and minor axes of 20 and 10 pixels. The kernel, <script type="math/tex">\mathbf{K}</script>, comprises two ellipse halves, <script type="math/tex">\mathbf{L_s}</script> and <script type="math/tex">\mathbf{R_s}</script>:</p>
<p>
<script type="math/tex; mode=display">\mathbf{K}_{s, L} (k) = \mathbf{L_s} (k) - \alpha_s \mathbf{R_s} (k) - \beta_s </script>
<script type="math/tex; mode=display">\mathbf{K}_{s, R} (k) = \mathbf{R_s} (k) - \alpha_s \mathbf{L_s} (k) - \beta_s</script>
</p>
<p><img alt="simple-cell-kernel-example" src="../../images/SegmentationAndGrouping/FBF/simple_cell_kernel.png" /></p>
<p>Here, <script type="math/tex">k</script> denotes the orientation index of the kernel, ranging from k_0 = 0, k_1 = 22.5, ... to k_7 = 157.5 degrees. For clarity, note that Grossberg and Wyse didn't just rotate the simple cell kernel by 360 degrees. Instead, they rotate two kernels, <script type="math/tex">\mathbf{L_s}</script> and <script type="math/tex">\mathbf{R_s}</script>, with opposite polarities by 180 degrees. This is because the kernels of the same orientation and opposite polarity will combine in the next step, making this notation more intuitive. The output of the simple cells comes from the cross-correlation between the ON-C (or OFF-C) signals, followed by half-wave rectification:</p>
<p>
<script type="math/tex; mode=display">\mathbf{S^+_{s, L}} (k) = \max(\mathbf{K}_{s, L} (k) \otimes \mathbf{x}, 0)</script>
<script type="math/tex; mode=display">\mathbf{S^+_{s, R}} (k) = \max(\mathbf{K}_{s, R} (k) \otimes \mathbf{x}, 0)</script>
<script type="math/tex; mode=display">\mathbf{S^-_{s, L}} (k) = \max(\mathbf{K}_{s, L} (k) \otimes \bar{\mathbf{x}}, 0)</script>
<script type="math/tex; mode=display">\mathbf{S^-_{s, R}} (k) = \max(\mathbf{K}_{s, R} (k) \otimes \bar{\mathbf{x}}, 0)</script>
</p>
</li>
<li>
<p><strong>Step 2b: Complex Cells</strong>: The complex cells combine the responses from simple cells of the same orientation. Be aware that the <script type="math/tex">\mathbf{C}</script> notation here represents the response map of complex cells, not the Gaussian from Step 1.</p>
<p>
<script type="math/tex; mode=display">\mathbf{C}_s (k) = F \left[ \mathbf{S^+_{s, L}} (k) + \mathbf{S^+_{s, R}} (k) + \mathbf{S^-_{s, L}} (k) + \mathbf{S^-_{s, R}} (k) \right]</script>
</p>
<p>Here, <script type="math/tex">F</script> is another constant parameter. Half-wave rectification isn't necessary as the output is always non-negative.</p>
</li>
<li>
<p><strong>Step 2c: Hypercomplex Cells (First Competitive Stage)</strong>: This step resembles the non-max suppression in Canny edge detection as it aims to inhibit texture around boundaries that lack other collinear edges. This is accomplished by dividing the complex cells response map by the cross-correlation between the complex cell response and the oriented competition kernel <script type="math/tex">\mathbf{G_s} (k)</script>. This kernel is normalized and positive everywhere except along the direction of the complex cell's corresponding orientation. Consequently, if there are non-collinear edges in the vicinity, the cross-correlation term in the denominator becomes large.</p>
<p><img alt="oriented-competition-kernel" src="../../images/SegmentationAndGrouping/FBF/oriented_competition_kernel.png" /></p>
<p>
<script type="math/tex; mode=display">\mathbf{D}_{s} (k) = \max \left[\frac{\mathbf{C_s}(k)}{\epsilon + \mu \sum_m (\mathbf{C_s} (m) \otimes \mathbf{G_s} (k))} - \tau, 0\right]</script>
</p>
</li>
<li>
<p><strong>Step 2d: Hypercomplex Cells (Second Competitive Stage)</strong>: In this stage, only the dominant orientation is preserved (winner-takes-all) for each location on the complex cell map, <script type="math/tex">\mathbf{C_s} (k)</script>.</p>
</li>
</ul>
<p>
<script type="math/tex; mode=display">\mathbf{D}_s := \max_k \mathbf{D}_s (k)</script>
</p>
<ul>
<li><strong>Step 2e: Multiple Scale Interaction: Boundary Localization and Noise Suppression</strong>: </li>
</ul>
</li>
</ul>
<h3 id="configr-carpenter-gaddam-and-mingolla-2007">CONFIGR (Carpenter, Gaddam, and Mingolla, 2007)</h3>
<ul>
<li><a href="https://pubmed.ncbi.nlm.nih.gov/18024082/">Paper</a></li>
</ul>
<h3 id="tolerance-space-theory-tst-for-gestalt-proximity-principle-peng-yang-and-li-2021">Tolerance Space Theory (TST) for Gestalt Proximity Principle (Peng, Yang, and Li, 2021)</h3>
<ul>
<li><a href="https://jov.arvojournals.org/article.aspx?articleid=2772625">Paper</a></li>
</ul>
<h3 id="gestalt-laws-models">Gestalt Laws Models</h3>
<h3 id="border-ownership-models">Border Ownership Models</h3>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../feature-extraction/" class="btn btn-neutral float-left" title="2. Feature Extraction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../object-detection-and-recognition/" class="btn btn-neutral float-right" title="4. Object Detection and Recognition">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright &copy; 2023 Tony Fu</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/tonyfu97/VisualAlgo/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../feature-extraction/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../object-detection-and-recognition/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
